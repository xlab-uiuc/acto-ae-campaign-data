19:56:53 ConfigMap 08db0feb.percona.com 833:	percona-server-mongodb-operator-67d4f7db5f-57pzw_71557f3c-25f0-4886-ad07-d86654af5880 became leader
19:56:53 Lease 08db0feb.percona.com 834:	percona-server-mongodb-operator-67d4f7db5f-57pzw_71557f3c-25f0-4886-ad07-d86654af5880 became leader
19:57:20 ConfigMap 08db0feb.percona.com 921:	percona-server-mongodb-operator-fb6cf9f7c-7ps96_c2c66a97-7c6f-4a48-bd4a-02cd33a03edd became leader
19:57:20 Lease 08db0feb.percona.com 922:	percona-server-mongodb-operator-fb6cf9f7c-7ps96_c2c66a97-7c6f-4a48-bd4a-02cd33a03edd became leader
19:57:22 PersistentVolumeClaim mongod-data-test-cluster-cfg-0 937:	waiting for first consumer to be created before binding
19:57:22 PersistentVolumeClaim mongod-data-test-cluster-cfg-0 944:	waiting for a volume to be created, either by external provisioner "rancher.io/local-path" or manually created by system administrator
19:57:22 PersistentVolumeClaim mongod-data-test-cluster-cfg-0 944:	External provisioner is provisioning volume for claim "acto-namespace/mongod-data-test-cluster-cfg-0"
19:57:27 PersistentVolumeClaim mongod-data-test-cluster-cfg-0 944:	Successfully provisioned volume pvc-299854b8-9c5a-41a2-9acc-77d9ad473682
19:57:42 PersistentVolumeClaim mongod-data-test-cluster-cfg-1 1253:	waiting for first consumer to be created before binding
19:57:42 PersistentVolumeClaim mongod-data-test-cluster-cfg-1 1260:	waiting for a volume to be created, either by external provisioner "rancher.io/local-path" or manually created by system administrator
19:57:42 PersistentVolumeClaim mongod-data-test-cluster-cfg-1 1260:	External provisioner is provisioning volume for claim "acto-namespace/mongod-data-test-cluster-cfg-1"
19:57:46 PersistentVolumeClaim mongod-data-test-cluster-cfg-1 1260:	Successfully provisioned volume pvc-a7da0b88-cb9f-4ff2-981c-90465ac91258
19:58:02 PersistentVolumeClaim mongod-data-test-cluster-cfg-2 1466:	waiting for first consumer to be created before binding
19:58:02 PersistentVolumeClaim mongod-data-test-cluster-cfg-2 1472:	waiting for a volume to be created, either by external provisioner "rancher.io/local-path" or manually created by system administrator
19:58:02 PersistentVolumeClaim mongod-data-test-cluster-cfg-2 1472:	External provisioner is provisioning volume for claim "acto-namespace/mongod-data-test-cluster-cfg-2"
19:58:06 PersistentVolumeClaim mongod-data-test-cluster-cfg-2 1472:	Successfully provisioned volume pvc-259e06e5-9d2c-4812-86a8-fba21931d605
19:57:23 PersistentVolumeClaim mongod-data-test-cluster-rs0-0 967:	waiting for first consumer to be created before binding
19:57:23 PersistentVolumeClaim mongod-data-test-cluster-rs0-0 988:	waiting for a volume to be created, either by external provisioner "rancher.io/local-path" or manually created by system administrator
19:57:23 PersistentVolumeClaim mongod-data-test-cluster-rs0-0 988:	External provisioner is provisioning volume for claim "acto-namespace/mongod-data-test-cluster-rs0-0"
19:57:28 PersistentVolumeClaim mongod-data-test-cluster-rs0-0 988:	Successfully provisioned volume pvc-1b8abbd5-f369-4327-81c8-c4e5bf7a0afc
19:57:41 PersistentVolumeClaim mongod-data-test-cluster-rs0-1 1227:	waiting for first consumer to be created before binding
19:57:41 PersistentVolumeClaim mongod-data-test-cluster-rs0-1 1234:	waiting for a volume to be created, either by external provisioner "rancher.io/local-path" or manually created by system administrator
19:57:41 PersistentVolumeClaim mongod-data-test-cluster-rs0-1 1234:	External provisioner is provisioning volume for claim "acto-namespace/mongod-data-test-cluster-rs0-1"
19:57:45 PersistentVolumeClaim mongod-data-test-cluster-rs0-1 1234:	Successfully provisioned volume pvc-38a82392-0fff-4a9d-9830-db6dd755718d
19:57:58 PersistentVolumeClaim mongod-data-test-cluster-rs0-2 1417:	waiting for first consumer to be created before binding
19:57:58 PersistentVolumeClaim mongod-data-test-cluster-rs0-2 1424:	waiting for a volume to be created, either by external provisioner "rancher.io/local-path" or manually created by system administrator
19:57:58 PersistentVolumeClaim mongod-data-test-cluster-rs0-2 1424:	External provisioner is provisioning volume for claim "acto-namespace/mongod-data-test-cluster-rs0-2"
19:58:02 PersistentVolumeClaim mongod-data-test-cluster-rs0-2 1424:	Successfully provisioned volume pvc-73453a16-3bbd-4c8c-b70a-da6bab119dca
19:58:15 PersistentVolumeClaim mongod-data-test-cluster-rs0-3 1569:	waiting for first consumer to be created before binding
19:58:15 PersistentVolumeClaim mongod-data-test-cluster-rs0-3 1577:	waiting for a volume to be created, either by external provisioner "rancher.io/local-path" or manually created by system administrator
19:58:15 PersistentVolumeClaim mongod-data-test-cluster-rs0-3 1577:	External provisioner is provisioning volume for claim "acto-namespace/mongod-data-test-cluster-rs0-3"
19:58:19 PersistentVolumeClaim mongod-data-test-cluster-rs0-3 1577:	Successfully provisioned volume pvc-463c47ed-98b0-4ab7-a4d6-d75d3c160d66
19:57:23 PersistentVolumeClaim mongod-data-test-cluster-rs0-nv-0 993:	waiting for first consumer to be created before binding
19:57:23 PersistentVolumeClaim mongod-data-test-cluster-rs0-nv-0 1008:	waiting for a volume to be created, either by external provisioner "rancher.io/local-path" or manually created by system administrator
19:57:23 PersistentVolumeClaim mongod-data-test-cluster-rs0-nv-0 1008:	External provisioner is provisioning volume for claim "acto-namespace/mongod-data-test-cluster-rs0-nv-0"
19:57:27 PersistentVolumeClaim mongod-data-test-cluster-rs0-nv-0 1008:	Successfully provisioned volume pvc-17dce65a-b8ff-4305-a603-038a7222e392
19:57:40 PersistentVolumeClaim mongod-data-test-cluster-rs0-nv-1 1199:	waiting for first consumer to be created before binding
19:57:40 PersistentVolumeClaim mongod-data-test-cluster-rs0-nv-1 1206:	waiting for a volume to be created, either by external provisioner "rancher.io/local-path" or manually created by system administrator
19:57:40 PersistentVolumeClaim mongod-data-test-cluster-rs0-nv-1 1206:	External provisioner is provisioning volume for claim "acto-namespace/mongod-data-test-cluster-rs0-nv-1"
19:57:44 PersistentVolumeClaim mongod-data-test-cluster-rs0-nv-1 1206:	Successfully provisioned volume pvc-3718ab31-9818-4468-9fb5-524caf02e882
19:56:50 Pod percona-server-mongodb-operator-67d4f7db5f-57pzw 806:	Successfully assigned acto-namespace/percona-server-mongodb-operator-67d4f7db5f-57pzw to acto-cluster-1-worker
19:56:51 Pod percona-server-mongodb-operator-67d4f7db5f-57pzw 809:	Container image "perconalab/percona-server-mongodb-operator:release-1-12-0" already present on machine
19:56:51 Pod percona-server-mongodb-operator-67d4f7db5f-57pzw 809:	Created container percona-server-mongodb-operator
19:56:51 Pod percona-server-mongodb-operator-67d4f7db5f-57pzw 809:	Started container percona-server-mongodb-operator
19:57:02 Pod percona-server-mongodb-operator-67d4f7db5f-57pzw 809:	Stopping container percona-server-mongodb-operator
19:56:50 ReplicaSet percona-server-mongodb-operator-67d4f7db5f 802:	Created pod: percona-server-mongodb-operator-67d4f7db5f-57pzw
19:57:02 ReplicaSet percona-server-mongodb-operator-67d4f7db5f 878:	Deleted pod: percona-server-mongodb-operator-67d4f7db5f-57pzw
19:57:00 Pod percona-server-mongodb-operator-fb6cf9f7c-7ps96 859:	Successfully assigned acto-namespace/percona-server-mongodb-operator-fb6cf9f7c-7ps96 to acto-cluster-1-worker2
19:57:01 Pod percona-server-mongodb-operator-fb6cf9f7c-7ps96 862:	Container image "perconalab/percona-server-mongodb-operator:release-1-12-0" already present on machine
19:57:01 Pod percona-server-mongodb-operator-fb6cf9f7c-7ps96 862:	Created container percona-server-mongodb-operator
19:57:01 Pod percona-server-mongodb-operator-fb6cf9f7c-7ps96 862:	Started container percona-server-mongodb-operator
19:57:00 ReplicaSet percona-server-mongodb-operator-fb6cf9f7c 857:	Created pod: percona-server-mongodb-operator-fb6cf9f7c-7ps96
19:56:50 Deployment percona-server-mongodb-operator 800:	Scaled up replica set percona-server-mongodb-operator-67d4f7db5f to 1
19:57:00 Deployment percona-server-mongodb-operator 856:	Scaled up replica set percona-server-mongodb-operator-fb6cf9f7c to 1
19:57:01 Deployment percona-server-mongodb-operator 867:	Scaled down replica set percona-server-mongodb-operator-67d4f7db5f to 0
19:57:23 PodDisruptionBudget test-cluster-arbiter-rs0 965:	No matching pods found
19:57:27 Pod test-cluster-cfg-0 939:	Successfully assigned acto-namespace/test-cluster-cfg-0 to acto-cluster-1-worker
19:57:28 Pod test-cluster-cfg-0 1079:	Container image "perconalab/percona-server-mongodb-operator:release-1-12-0" already present on machine
19:57:28 Pod test-cluster-cfg-0 1079:	Created container mongo-init
19:57:29 Pod test-cluster-cfg-0 1079:	Started container mongo-init
19:57:29 Pod test-cluster-cfg-0 1079:	Container image "percona/percona-server-mongodb:4.4.10-11" already present on machine
19:57:30 Pod test-cluster-cfg-0 1079:	Created container mongod
19:57:30 Pod test-cluster-cfg-0 1079:	Started container mongod
19:57:30 Pod test-cluster-cfg-0 1079:	Pulling image "percona/percona-backup-mongodb:1.7.0"
19:57:35 Pod test-cluster-cfg-0 1079:	Successfully pulled image "percona/percona-backup-mongodb:1.7.0" in 5.099794396s
19:57:35 Pod test-cluster-cfg-0 1079:	Created container backup-agent
19:57:35 Pod test-cluster-cfg-0 1079:	Started container backup-agent
19:57:47 Pod test-cluster-cfg-1 1256:	Successfully assigned acto-namespace/test-cluster-cfg-1 to acto-cluster-1-worker3
19:57:48 Pod test-cluster-cfg-1 1346:	Container image "perconalab/percona-server-mongodb-operator:release-1-12-0" already present on machine
19:57:48 Pod test-cluster-cfg-1 1346:	Created container mongo-init
19:57:49 Pod test-cluster-cfg-1 1346:	Started container mongo-init
19:57:50 Pod test-cluster-cfg-1 1346:	Container image "percona/percona-server-mongodb:4.4.10-11" already present on machine
19:57:50 Pod test-cluster-cfg-1 1346:	Created container mongod
19:57:50 Pod test-cluster-cfg-1 1346:	Started container mongod
19:57:50 Pod test-cluster-cfg-1 1346:	Container image "percona/percona-backup-mongodb:1.7.0" already present on machine
19:57:50 Pod test-cluster-cfg-1 1346:	Created container backup-agent
19:57:50 Pod test-cluster-cfg-1 1346:	Started container backup-agent
19:58:07 Pod test-cluster-cfg-2 1469:	Successfully assigned acto-namespace/test-cluster-cfg-2 to acto-cluster-1-worker4
19:58:08 Pod test-cluster-cfg-2 1525:	Container image "perconalab/percona-server-mongodb-operator:release-1-12-0" already present on machine
19:58:08 Pod test-cluster-cfg-2 1525:	Created container mongo-init
19:58:08 Pod test-cluster-cfg-2 1525:	Started container mongo-init
19:58:09 Pod test-cluster-cfg-2 1525:	Container image "percona/percona-server-mongodb:4.4.10-11" already present on machine
19:58:09 Pod test-cluster-cfg-2 1525:	Created container mongod
19:58:09 Pod test-cluster-cfg-2 1525:	Started container mongod
19:58:09 Pod test-cluster-cfg-2 1525:	Container image "percona/percona-backup-mongodb:1.7.0" already present on machine
19:58:09 Pod test-cluster-cfg-2 1525:	Created container backup-agent
19:58:09 Pod test-cluster-cfg-2 1525:	Started container backup-agent
19:57:22 StatefulSet test-cluster-cfg 935:	create Claim mongod-data-test-cluster-cfg-0 Pod test-cluster-cfg-0 in StatefulSet test-cluster-cfg success
19:57:22 StatefulSet test-cluster-cfg 935:	create Pod test-cluster-cfg-0 in StatefulSet test-cluster-cfg successful
19:57:42 StatefulSet test-cluster-cfg 943:	create Claim mongod-data-test-cluster-cfg-1 Pod test-cluster-cfg-1 in StatefulSet test-cluster-cfg success
19:57:42 StatefulSet test-cluster-cfg 943:	create Pod test-cluster-cfg-1 in StatefulSet test-cluster-cfg successful
19:58:02 StatefulSet test-cluster-cfg 1257:	create Claim mongod-data-test-cluster-cfg-2 Pod test-cluster-cfg-2 in StatefulSet test-cluster-cfg success
19:58:02 StatefulSet test-cluster-cfg 1257:	create Pod test-cluster-cfg-2 in StatefulSet test-cluster-cfg successful
19:57:23 PodDisruptionBudget test-cluster-mongod-rs0 958:	No matching pods found
19:58:38 PodDisruptionBudget test-cluster-mongos- 1696:	No matching pods found
19:58:38 Pod test-cluster-mongos-0 1705:	Successfully assigned acto-namespace/test-cluster-mongos-0 to acto-cluster-1-worker2
19:58:38 Pod test-cluster-mongos-0 1708:	Container image "perconalab/percona-server-mongodb-operator:release-1-12-0" already present on machine
19:58:38 Pod test-cluster-mongos-0 1708:	Created container mongo-init
19:58:39 Pod test-cluster-mongos-0 1708:	Started container mongo-init
19:58:40 Pod test-cluster-mongos-0 1708:	Container image "percona/percona-server-mongodb:4.4.10-11" already present on machine
19:58:40 Pod test-cluster-mongos-0 1708:	Created container mongos
19:58:40 Pod test-cluster-mongos-0 1708:	Started container mongos
19:58:50 Pod test-cluster-mongos-1 1762:	Successfully assigned acto-namespace/test-cluster-mongos-1 to acto-cluster-1-worker
19:58:50 Pod test-cluster-mongos-1 1764:	Container image "perconalab/percona-server-mongodb-operator:release-1-12-0" already present on machine
19:58:50 Pod test-cluster-mongos-1 1764:	Created container mongo-init
19:58:51 Pod test-cluster-mongos-1 1764:	Started container mongo-init
19:58:52 Pod test-cluster-mongos-1 1764:	Container image "percona/percona-server-mongodb:4.4.10-11" already present on machine
19:58:52 Pod test-cluster-mongos-1 1764:	Created container mongos
19:58:52 Pod test-cluster-mongos-1 1764:	Started container mongos
19:58:38 StatefulSet test-cluster-mongos 1695:	create Pod test-cluster-mongos-0 in StatefulSet test-cluster-mongos successful
19:58:50 StatefulSet test-cluster-mongos 1710:	create Pod test-cluster-mongos-1 in StatefulSet test-cluster-mongos successful
19:57:23 PodDisruptionBudget test-cluster-nonVoting-rs0 975:	No matching pods found
19:57:29 Pod test-cluster-rs0-0 974:	Successfully assigned acto-namespace/test-cluster-rs0-0 to acto-cluster-1-worker4
19:57:29 Pod test-cluster-rs0-0 1107:	Container image "perconalab/percona-server-mongodb-operator:release-1-12-0" already present on machine
19:57:29 Pod test-cluster-rs0-0 1107:	Created container mongo-init
19:57:30 Pod test-cluster-rs0-0 1107:	Started container mongo-init
19:57:30 Pod test-cluster-rs0-0 1107:	Container image "percona/percona-server-mongodb:4.4.10-11" already present on machine
19:57:30 Pod test-cluster-rs0-0 1107:	Created container mongod
19:57:31 Pod test-cluster-rs0-0 1107:	Started container mongod
19:57:31 Pod test-cluster-rs0-0 1107:	Pulling image "percona/percona-backup-mongodb:1.7.0"
19:57:37 Pod test-cluster-rs0-0 1107:	Successfully pulled image "percona/percona-backup-mongodb:1.7.0" in 5.80689855s
19:57:37 Pod test-cluster-rs0-0 1107:	Created container backup-agent
19:57:37 Pod test-cluster-rs0-0 1107:	Started container backup-agent
19:57:46 Pod test-cluster-rs0-1 1230:	Successfully assigned acto-namespace/test-cluster-rs0-1 to acto-cluster-1-worker3
19:57:47 Pod test-cluster-rs0-1 1318:	Container image "perconalab/percona-server-mongodb-operator:release-1-12-0" already present on machine
19:57:47 Pod test-cluster-rs0-1 1318:	Created container mongo-init
19:57:47 Pod test-cluster-rs0-1 1318:	Started container mongo-init
19:57:48 Pod test-cluster-rs0-1 1318:	Container image "percona/percona-server-mongodb:4.4.10-11" already present on machine
19:57:48 Pod test-cluster-rs0-1 1318:	Created container mongod
19:57:48 Pod test-cluster-rs0-1 1318:	Started container mongod
19:57:48 Pod test-cluster-rs0-1 1318:	Container image "percona/percona-backup-mongodb:1.7.0" already present on machine
19:57:48 Pod test-cluster-rs0-1 1318:	Created container backup-agent
19:57:48 Pod test-cluster-rs0-1 1318:	Started container backup-agent
19:58:03 Pod test-cluster-rs0-2 1420:	Successfully assigned acto-namespace/test-cluster-rs0-2 to acto-cluster-1-worker2
19:58:03 Pod test-cluster-rs0-2 1479:	Container image "perconalab/percona-server-mongodb-operator:release-1-12-0" already present on machine
19:58:03 Pod test-cluster-rs0-2 1479:	Created container mongo-init
19:58:04 Pod test-cluster-rs0-2 1479:	Started container mongo-init
19:58:05 Pod test-cluster-rs0-2 1479:	Container image "percona/percona-server-mongodb:4.4.10-11" already present on machine
19:58:05 Pod test-cluster-rs0-2 1479:	Created container mongod
19:58:05 Pod test-cluster-rs0-2 1479:	Started container mongod
19:58:05 Pod test-cluster-rs0-2 1479:	Pulling image "percona/percona-backup-mongodb:1.7.0"
19:58:10 Pod test-cluster-rs0-2 1479:	Successfully pulled image "percona/percona-backup-mongodb:1.7.0" in 4.491983921s
19:58:10 Pod test-cluster-rs0-2 1479:	Created container backup-agent
19:58:10 Pod test-cluster-rs0-2 1479:	Started container backup-agent
19:58:20 Pod test-cluster-rs0-3 1574:	Successfully assigned acto-namespace/test-cluster-rs0-3 to acto-cluster-1-worker
19:58:20 Pod test-cluster-rs0-3 1618:	Container image "perconalab/percona-server-mongodb-operator:release-1-12-0" already present on machine
19:58:20 Pod test-cluster-rs0-3 1618:	Created container mongo-init
19:58:21 Pod test-cluster-rs0-3 1618:	Started container mongo-init
19:58:22 Pod test-cluster-rs0-3 1618:	Container image "percona/percona-server-mongodb:4.4.10-11" already present on machine
19:58:22 Pod test-cluster-rs0-3 1618:	Created container mongod
19:58:22 Pod test-cluster-rs0-3 1618:	Started container mongod
19:58:22 Pod test-cluster-rs0-3 1618:	Container image "percona/percona-backup-mongodb:1.7.0" already present on machine
19:58:22 Pod test-cluster-rs0-3 1618:	Created container backup-agent
19:58:22 Pod test-cluster-rs0-3 1618:	Started container backup-agent
20:00:12 Pod test-cluster-rs0-3 1618:	Stopping container mongod
20:00:12 Pod test-cluster-rs0-3 1618:	Stopping container backup-agent
20:10:05 Pod test-cluster-rs0-3 3585:	Successfully assigned acto-namespace/test-cluster-rs0-3 to acto-cluster-1-worker
20:10:05 Pod test-cluster-rs0-3 3587:	Container image "perconalab/percona-server-mongodb-operator:release-1-12-0" already present on machine
20:10:05 Pod test-cluster-rs0-3 3587:	Created container mongo-init
20:10:06 Pod test-cluster-rs0-3 3587:	Started container mongo-init
20:10:06 Pod test-cluster-rs0-3 3587:	Container image "percona/percona-server-mongodb:4.4.10-11" already present on machine
20:10:06 Pod test-cluster-rs0-3 3587:	Created container mongod
20:10:06 Pod test-cluster-rs0-3 3587:	Started container mongod
20:10:06 Pod test-cluster-rs0-3 3587:	Container image "percona/percona-backup-mongodb:1.7.0" already present on machine
20:10:06 Pod test-cluster-rs0-3 3587:	Created container backup-agent
20:10:07 Pod test-cluster-rs0-3 3587:	Started container backup-agent
19:57:23 Pod test-cluster-rs0-arbiter-0 976:	Successfully assigned acto-namespace/test-cluster-rs0-arbiter-0 to acto-cluster-1-worker2
19:57:23 Pod test-cluster-rs0-arbiter-0 983:	Container image "perconalab/percona-server-mongodb-operator:release-1-12-0" already present on machine
19:57:23 Pod test-cluster-rs0-arbiter-0 983:	Created container mongo-init
19:57:24 Pod test-cluster-rs0-arbiter-0 983:	Started container mongo-init
19:57:25 Pod test-cluster-rs0-arbiter-0 983:	Container image "percona/percona-server-mongodb:4.4.10-11" already present on machine
19:57:25 Pod test-cluster-rs0-arbiter-0 983:	Created container mongod-arbiter
19:57:25 Pod test-cluster-rs0-arbiter-0 983:	Started container mongod-arbiter
19:57:23 StatefulSet test-cluster-rs0-arbiter 963:	create Pod test-cluster-rs0-arbiter-0 in StatefulSet test-cluster-rs0-arbiter successful
19:57:28 Pod test-cluster-rs0-nv-0 1000:	Successfully assigned acto-namespace/test-cluster-rs0-nv-0 to acto-cluster-1-worker3
19:57:29 Pod test-cluster-rs0-nv-0 1090:	Container image "perconalab/percona-server-mongodb-operator:release-1-12-0" already present on machine
19:57:29 Pod test-cluster-rs0-nv-0 1090:	Created container mongo-init
19:57:29 Pod test-cluster-rs0-nv-0 1090:	Started container mongo-init
19:57:30 Pod test-cluster-rs0-nv-0 1090:	Container image "percona/percona-server-mongodb:4.4.10-11" already present on machine
19:57:30 Pod test-cluster-rs0-nv-0 1090:	Created container mongod-nv
19:57:30 Pod test-cluster-rs0-nv-0 1090:	Started container mongod-nv
19:57:30 Pod test-cluster-rs0-nv-0 1090:	Pulling image "percona/percona-backup-mongodb:1.7.0"
19:57:35 Pod test-cluster-rs0-nv-0 1090:	Successfully pulled image "percona/percona-backup-mongodb:1.7.0" in 5.336034323s
19:57:35 Pod test-cluster-rs0-nv-0 1090:	Created container backup-agent
19:57:36 Pod test-cluster-rs0-nv-0 1090:	Started container backup-agent
19:57:45 Pod test-cluster-rs0-nv-1 1202:	Successfully assigned acto-namespace/test-cluster-rs0-nv-1 to acto-cluster-1-worker4
19:57:45 Pod test-cluster-rs0-nv-1 1307:	Container image "perconalab/percona-server-mongodb-operator:release-1-12-0" already present on machine
19:57:45 Pod test-cluster-rs0-nv-1 1307:	Created container mongo-init
19:57:46 Pod test-cluster-rs0-nv-1 1307:	Started container mongo-init
19:57:46 Pod test-cluster-rs0-nv-1 1307:	Container image "percona/percona-server-mongodb:4.4.10-11" already present on machine
19:57:47 Pod test-cluster-rs0-nv-1 1307:	Created container mongod-nv
19:57:47 Pod test-cluster-rs0-nv-1 1307:	Started container mongod-nv
19:57:47 Pod test-cluster-rs0-nv-1 1307:	Container image "percona/percona-backup-mongodb:1.7.0" already present on machine
19:57:47 Pod test-cluster-rs0-nv-1 1307:	Created container backup-agent
19:57:47 Pod test-cluster-rs0-nv-1 1307:	Started container backup-agent
19:57:23 StatefulSet test-cluster-rs0-nv 972:	create Claim mongod-data-test-cluster-rs0-nv-0 Pod test-cluster-rs0-nv-0 in StatefulSet test-cluster-rs0-nv success
19:57:23 StatefulSet test-cluster-rs0-nv 972:	create Pod test-cluster-rs0-nv-0 in StatefulSet test-cluster-rs0-nv successful
19:57:40 StatefulSet test-cluster-rs0-nv 1005:	create Claim mongod-data-test-cluster-rs0-nv-1 Pod test-cluster-rs0-nv-1 in StatefulSet test-cluster-rs0-nv success
19:57:40 StatefulSet test-cluster-rs0-nv 1005:	create Pod test-cluster-rs0-nv-1 in StatefulSet test-cluster-rs0-nv successful
19:57:23 StatefulSet test-cluster-rs0 956:	create Claim mongod-data-test-cluster-rs0-0 Pod test-cluster-rs0-0 in StatefulSet test-cluster-rs0 success
19:57:23 StatefulSet test-cluster-rs0 956:	create Pod test-cluster-rs0-0 in StatefulSet test-cluster-rs0 successful
19:57:41 StatefulSet test-cluster-rs0 984:	create Claim mongod-data-test-cluster-rs0-1 Pod test-cluster-rs0-1 in StatefulSet test-cluster-rs0 success
19:57:41 StatefulSet test-cluster-rs0 984:	create Pod test-cluster-rs0-1 in StatefulSet test-cluster-rs0 successful
19:57:58 StatefulSet test-cluster-rs0 1233:	create Claim mongod-data-test-cluster-rs0-2 Pod test-cluster-rs0-2 in StatefulSet test-cluster-rs0 success
19:57:58 StatefulSet test-cluster-rs0 1233:	create Pod test-cluster-rs0-2 in StatefulSet test-cluster-rs0 successful
19:58:15 StatefulSet test-cluster-rs0 1423:	create Claim mongod-data-test-cluster-rs0-3 Pod test-cluster-rs0-3 in StatefulSet test-cluster-rs0 success
19:58:15 StatefulSet test-cluster-rs0 1423:	create Pod test-cluster-rs0-3 in StatefulSet test-cluster-rs0 successful
20:00:15 StatefulSet test-cluster-rs0 2012:	create Pod test-cluster-rs0-3 in StatefulSet test-cluster-rs0 failed error: Pod "test-cluster-rs0-3" is invalid: [spec.containers[1].name: Invalid value: "ACTOKEY": a lowercase RFC 1123 label must consist of lower case alphanumeric characters or '-', and must start and end with an alphanumeric character (e.g. 'my-name',  or '123-abc', regex used for validation is '[a-z0-9]([-a-z0-9]*[a-z0-9])?'), spec.containers[1].lifecycle.postStart.httpGet.port: Invalid value: "ACTOKEY": must contain only alpha-numeric characters (a-z, 0-9), and hyphens (-), spec.containers[1].lifecycle.postStart.httpGet.port: Invalid value: "ACTOKEY": must contain at least one letter or number (a-z, 0-9), spec.containers[1].lifecycle.postStart.httpGet.scheme: Unsupported value: "ACTOKEY": supported values: "HTTP", "HTTPS", spec.containers[1].lifecycle.postStart.tcpSocket: Forbidden: may not specify more than 1 handler type, spec.containers[1].lifecycle.preStop.exec.command: Required value, spec.containers[1].lifecycle.preStop.httpGet: Forbidden: may not specify more than 1 handler type, spec.containers[1].lifecycle.preStop.tcpSocket: Forbidden: may not specify more than 1 handler type, spec.containers[1].livenessProbe.httpGet: Forbidden: may not specify more than 1 handler type, spec.containers[1].livenessProbe.tcpSocket: Forbidden: may not specify more than 1 handler type, spec.containers[1].startupProbe.exec.command: Required value, spec.containers[1].startupProbe.httpGet: Forbidden: may not specify more than 1 handler type, spec.containers[1].startupProbe.tcpSocket: Forbidden: may not specify more than 1 handler type, spec.containers[1].livenessProbe.successThreshold: Invalid value: 5: must be 1, spec.containers[1].startupProbe.successThreshold: Invalid value: 4: must be 1, spec.containers[1].terminationMessagePolicy: Invalid value: "ACTOKEY": must be 'File' or 'FallbackToLogsOnError', spec.containers[1].ports[0].name: Invalid value: "ACTOKEY": must contain only alpha-numeric characters (a-z, 0-9), and hyphens (-), spec.containers[1].ports[0].name: Invalid value: "ACTOKEY": must contain at least one letter or number (a-z, 0-9), spec.containers[1].ports[0].protocol: Unsupported value: "ACTOKEY": supported values: "SCTP", "TCP", "UDP", spec.containers[1].ports[1].name: Invalid value: "ACTOKEY": must contain only alpha-numeric characters (a-z, 0-9), and hyphens (-), spec.containers[1].ports[1].name: Invalid value: "ACTOKEY": must contain at least one letter or number (a-z, 0-9), spec.containers[1].ports[1].containerPort: Required value, spec.containers[1].ports[1].protocol: Unsupported value: "ACTOKEY": supported values: "SCTP", "TCP", "UDP", spec.containers[1].env[0].valueFrom.fieldRef.fieldPath: Invalid value: "ACTOKEY": error converting fieldPath: unsupported pod version: ACTOKEY, spec.containers[1].env[0].valueFrom.resourceFieldRef.resource: Unsupported value: "ACTOKEY": supported values: "limits.cpu", "limits.ephemeral-storage", "limits.memory", "requests.cpu", "requests.ephemeral-storage", "requests.memory", spec.containers[1].env[0].valueFrom.configMapKeyRef.name: Invalid value: "ACTOKEY": a lowercase RFC 1123 subdomain must consist of lower case alphanumeric characters, '-' or '.', and must start and end with an alphanumeric character (e.g. 'example.com', regex used for validation is '[a-z0-9]([-a-z0-9]*[a-z0-9])?(\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*'), spec.containers[1].env[0].valueFrom.secretKeyRef.name: Invalid value: "ACTOKEY": a lowercase RFC 1123 subdomain must consist of lower case alphanumeric characters, '-' or '.', and must start and end with an alphanumeric character (e.g. 'example.com', regex used for validation is '[a-z0-9]([-a-z0-9]*[a-z0-9])?(\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*'), spec.containers[1].env[0].valueFrom: Invalid value: "": may not be specified when `value` is not empty, spec.containers[1].env[1].valueFrom.fieldRef.fieldPath: Invalid value: "ACTOKEY": error converting fieldPath: unsupported pod version: ACTOKEY, spec.containers[1].env[1].valueFrom.resourceFieldRef.resource: Unsupported value: "ACTOKEY": supported values: "limits.cpu", "limits.ephemeral-storage", "limits.memory", "requests.cpu", "requests.ephemeral-storage", "requests.memory", spec.containers[1].env[1].valueFrom.configMapKeyRef.name: Invalid value: "ACTOKEY": a lowercase RFC 1123 subdomain must consist of lower case alphanumeric characters, '-' or '.', and must start and end with an alphanumeric character (e.g. 'example.com', regex used for validation is '[a-z0-9]([-a-z0-9]*[a-z0-9])?(\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*'), spec.containers[1].env[1].valueFrom.secretKeyRef.name: Invalid value: "": a lowercase RFC 1123 subdomain must consist of lower case alphanumeric characters, '-' or '.', and must start and end with an alphanumeric character (e.g. 'example.com', regex used for validation is '[a-z0-9]([-a-z0-9]*[a-z0-9])?(\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*'), spec.containers[1].env[1].valueFrom: Invalid value: "": may not be specified when `value` is not empty, spec.containers[1].env[2].valueFrom.fieldRef.fieldPath: Invalid value: "ACTOKEY": error converting fieldPath: unsupported pod version: ACTOKEY, spec.containers[1].env[2].valueFrom.resourceFieldRef.resource: Unsupported value: "ACTOKEY": supported values: "limits.cpu", "limits.ephemeral-storage", "limits.memory", "requests.cpu", "requests.ephemeral-storage", "requests.memory", spec.containers[1].env[2].valueFrom.configMapKeyRef.name: Invalid value: "ACTOKEY": a lowercase RFC 1123 subdomain must consist of lower case alphanumeric characters, '-' or '.', and must start and end with an alphanumeric character (e.g. 'example.com', regex used for validation is '[a-z0-9]([-a-z0-9]*[a-z0-9])?(\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*'), spec.containers[1].env[2].valueFrom.secretKeyRef.name: Invalid value: "ACTOKEY": a lowercase RFC 1123 subdomain must consist of lower case alphanumeric characters, '-' or '.', and must start and end with an alphanumeric character (e.g. 'example.com', regex used for validation is '[a-z0-9]([-a-z0-9]*[a-z0-9])?(\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*'), spec.containers[1].env[2].valueFrom: Invalid value: "": may not be specified when `value` is not empty, spec.containers[1].envFrom[0].configMapRef.name: Invalid value: "ACTOKEY": a lowercase RFC 1123 subdomain must consist of lower case alphanumeric characters, '-' or '.', and must start and end with an alphanumeric character (e.g. 'example.com', regex used for validation is '[a-z0-9]([-a-z0-9]*[a-z0-9])?(\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*'), spec.containers[1].envFrom[0].secretRef.name: Invalid value: "ACTOKEY": a lowercase RFC 1123 subdomain must consist of lower case alphanumeric characters, '-' or '.', and must start and end with an alphanumeric character (e.g. 'example.com', regex used for validation is '[a-z0-9]([-a-z0-9]*[a-z0-9])?(\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*'), spec.containers[1].envFrom: Invalid value: "": may not have more than one field specified at a time, spec.containers[1].envFrom[1].configMapRef.name: Invalid value: "ACTOKEY": a lowercase RFC 1123 subdomain must consist of lower case alphanumeric characters, '-' or '.', and must start and end with an alphanumeric character (e.g. 'example.com', regex used for validation is '[a-z0-9]([-a-z0-9]*[a-z0-9])?(\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*'), spec.containers[1].envFrom[1].secretRef.name: Invalid value: "ACTOKEY": a lowercase RFC 1123 subdomain must consist of lower case alphanumeric characters, '-' or '.', and must start and end with an alphanumeric character (e.g. 'example.com', regex used for validation is '[a-z0-9]([-a-z0-9]*[a-z0-9])?(\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*'), spec.containers[1].envFrom[2].configMapRef.name: Invalid value: "ACTOKEY": a lowercase RFC 1123 subdomain must consist of lower case alphanumeric characters, '-' or '.', and must start and end with an alphanumeric character (e.g. 'example.com', regex used for validation is '[a-z0-9]([-a-z0-9]*[a-z0-9])?(\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*'), spec.containers[1].envFrom[2].secretRef.name: Invalid value: "ACTOKEY": a lowercase RFC 1123 subdomain must consist of lower case alphanumeric characters, '-' or '.', and must start and end with an alphanumeric character (e.g. 'example.com', regex used for validation is '[a-z0-9]([-a-z0-9]*[a-z0-9])?(\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*'), spec.containers[1].envFrom[3].configMapRef.name: Invalid value: "ACTOKEY": a lowercase RFC 1123 subdomain must consist of lower case alphanumeric characters, '-' or '.', and must start and end with an alphanumeric character (e.g. 'example.com', regex used for validation is '[a-z0-9]([-a-z0-9]*[a-z0-9])?(\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*'), spec.containers[1].envFrom[3].secretRef.name: Invalid value: "ACTOKEY": a lowercase RFC 1123 subdomain must consist of lower case alphanumeric characters, '-' or '.', and must start and end with an alphanumeric character (e.g. 'example.com', regex used for validation is '[a-z0-9]([-a-z0-9]*[a-z0-9])?(\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*'), spec.containers[1].volumeMounts[0].name: Not found: "ACTOKEY", spec.containers[1].volumeMounts[0].name: Invalid value: "ACTOKEY": must not already exist in volumeDevices, spec.containers[1].volumeMounts[0].mountPath: Invalid value: "ACTOKEY": must not already exist as a path in volumeDevices, spec.containers[1].volumeMounts[0].subPathExpr: Invalid value: "ACTOKEY": subPathExpr and subPath are mutually exclusive, spec.containers[1].volumeMounts.mountPropagation: Unsupported value: "ACTOKEY": supported values: "Bidirectional", "HostToContainer", "None", spec.containers[1].volumeMounts[1].name: Not found: "ACTOKEY", spec.containers[1].volumeMounts[1].mountPath: Invalid value: "ACTOKEY": must be unique, spec.containers[1].volumeMounts[1].name: Invalid value: "ACTOKEY": must not already exist in volumeDevices, spec.containers[1].volumeMounts[1].mountPath: Invalid value: "ACTOKEY": must not already exist as a path in volumeDevices, spec.containers[1].volumeMounts[1].subPathExpr: Invalid value: "ACTOKEY": subPathExpr and subPath are mutually exclusive, spec.containers[1].volumeMounts[2].name: Not found: "ACTOKEY", spec.containers[1].volumeMounts[2].mountPath: Invalid value: "ACTOKEY": must be unique, spec.containers[1].volumeMounts[2].name: Invalid value: "ACTOKEY": must not already exist in volumeDevices, spec.containers[1].volumeMounts[2].mountPath: Invalid value: "ACTOKEY": must not already exist as a path in volumeDevices, spec.containers[1].volumeMounts[2].subPathExpr: Invalid value: "ACTOKEY": subPathExpr and subPath are mutually exclusive, spec.containers[1].volumeMounts[3].name: Not found: "ACTOKEY", spec.containers[1].volumeMounts[3].mountPath: Invalid value: "ACTOKEY": must be unique, spec.containers[1].volumeMounts[3].name: Invalid value: "ACTOKEY": must not already exist in volumeDevices, spec.containers[1].volumeMounts[3].mountPath: Invalid value: "ACTOKEY": must not already exist as a path in volumeDevices, spec.containers[1].volumeMounts[3].subPathExpr: Invalid value: "ACTOKEY": subPathExpr and subPath are mutually exclusive, spec.containers[1].volumeDevices[0].name: Not found: "ACTOKEY", spec.containers[1].volumeDevices[0].name: Invalid value: "ACTOKEY": must not already exist in volumeMounts, spec.containers[1].volumeDevices[0].devicePath: Invalid value: "ACTOKEY": must not already exist as a path in volumeMounts, spec.containers[1].volumeDevices[1].name: Invalid value: "ACTOKEY": must be unique, spec.containers[1].volumeDevices[1].name: Not found: "ACTOKEY", spec.containers[1].volumeDevices[1].devicePath: Invalid value: "ACTOKEY": must be unique, spec.containers[1].volumeDevices[1].name: Invalid value: "ACTOKEY": must not already exist in volumeMounts, spec.containers[1].volumeDevices[1].devicePath: Invalid value: "ACTOKEY": must not already exist as a path in volumeMounts, spec.containers[1].volumeDevices[2].name: Invalid value: "ACTOKEY": must be unique, spec.containers[1].volumeDevices[2].name: Not found: "ACTOKEY", spec.containers[1].volumeDevices[2].devicePath: Invalid value: "ACTOKEY": must be unique, spec.containers[1].volumeDevices[2].name: Invalid value: "ACTOKEY": must not already exist in volumeMounts, spec.containers[1].volumeDevices[2].devicePath: Invalid value: "ACTOKEY": must not already exist as a path in volumeMounts, spec.containers[1].imagePullPolicy: Unsupported value: "ACTOKEY": supported values: "Always", "IfNotPresent", "Never"]
