17:14:20 ConfigMap 08db1feb.percona.com 827:	percona-xtradb-cluster-operator-6db54dc756-vdz5v_668d930b-046a-4e79-8363-4cdfc8b0ce25 became leader
17:14:20 Lease 08db1feb.percona.com 828:	percona-xtradb-cluster-operator-6db54dc756-vdz5v_668d930b-046a-4e79-8363-4cdfc8b0ce25 became leader
17:14:51 ConfigMap 08db1feb.percona.com 929:	percona-xtradb-cluster-operator-64c4fd5974-kx465_980b9449-1044-468c-bf1a-3243b737fb71 became leader
17:14:51 Lease 08db1feb.percona.com 930:	percona-xtradb-cluster-operator-64c4fd5974-kx465_980b9449-1044-468c-bf1a-3243b737fb71 became leader
17:20:27 PersistentVolumeClaim datadir-test-cluster-pxc-0 1778:	waiting for first consumer to be created before binding
17:20:27 PersistentVolumeClaim datadir-test-cluster-pxc-0 1788:	waiting for a volume to be created, either by external provisioner "rancher.io/local-path" or manually created by system administrator
17:20:27 PersistentVolumeClaim datadir-test-cluster-pxc-0 1788:	External provisioner is provisioning volume for claim "acto-namespace/datadir-test-cluster-pxc-0"
17:20:32 PersistentVolumeClaim datadir-test-cluster-pxc-0 1788:	Successfully provisioned volume pvc-f739c983-8020-4d84-988e-1f445ecd9c06
17:21:33 PersistentVolumeClaim datadir-test-cluster-pxc-1 2085:	waiting for first consumer to be created before binding
17:21:33 PersistentVolumeClaim datadir-test-cluster-pxc-1 2092:	waiting for a volume to be created, either by external provisioner "rancher.io/local-path" or manually created by system administrator
17:21:33 PersistentVolumeClaim datadir-test-cluster-pxc-1 2092:	External provisioner is provisioning volume for claim "acto-namespace/datadir-test-cluster-pxc-1"
17:21:37 PersistentVolumeClaim datadir-test-cluster-pxc-1 2092:	Successfully provisioned volume pvc-d2abb6f6-ba2f-40eb-9b79-110bc4785fed
17:22:39 PersistentVolumeClaim datadir-test-cluster-pxc-2 2387:	waiting for first consumer to be created before binding
17:22:39 PersistentVolumeClaim datadir-test-cluster-pxc-2 2394:	waiting for a volume to be created, either by external provisioner "rancher.io/local-path" or manually created by system administrator
17:22:39 PersistentVolumeClaim datadir-test-cluster-pxc-2 2394:	External provisioner is provisioning volume for claim "acto-namespace/datadir-test-cluster-pxc-2"
17:22:44 PersistentVolumeClaim datadir-test-cluster-pxc-2 2394:	Successfully provisioned volume pvc-efa4cb98-d7ec-448d-a217-22320fd67b42
17:14:22 Pod percona-xtradb-cluster-operator-64c4fd5974-kx465 844:	Successfully assigned acto-namespace/percona-xtradb-cluster-operator-64c4fd5974-kx465 to acto-cluster-4-worker2
17:14:23 Pod percona-xtradb-cluster-operator-64c4fd5974-kx465 848:	Pulling image "percona/percona-xtradb-cluster-operator:1.11.0"
17:14:23 Pod percona-xtradb-cluster-operator-64c4fd5974-kx465 848:	Successfully pulled image "percona/percona-xtradb-cluster-operator:1.11.0" in 568.001775ms
17:14:23 Pod percona-xtradb-cluster-operator-64c4fd5974-kx465 848:	Created container percona-xtradb-cluster-operator
17:14:24 Pod percona-xtradb-cluster-operator-64c4fd5974-kx465 848:	Started container percona-xtradb-cluster-operator
17:14:33 Pod percona-xtradb-cluster-operator-64c4fd5974-kx465 848:	Liveness probe failed: Get "http://10.244.3.2:8080/metrics": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
17:14:22 ReplicaSet percona-xtradb-cluster-operator-64c4fd5974 842:	Created pod: percona-xtradb-cluster-operator-64c4fd5974-kx465
17:14:11 Pod percona-xtradb-cluster-operator-6db54dc756-vdz5v 786:	Successfully assigned acto-namespace/percona-xtradb-cluster-operator-6db54dc756-vdz5v to acto-cluster-4-worker
17:14:12 Pod percona-xtradb-cluster-operator-6db54dc756-vdz5v 789:	Pulling image "percona/percona-xtradb-cluster-operator:1.11.0"
17:14:13 Pod percona-xtradb-cluster-operator-6db54dc756-vdz5v 789:	Successfully pulled image "percona/percona-xtradb-cluster-operator:1.11.0" in 593.629106ms
17:14:13 Pod percona-xtradb-cluster-operator-6db54dc756-vdz5v 789:	Created container percona-xtradb-cluster-operator
17:14:14 Pod percona-xtradb-cluster-operator-6db54dc756-vdz5v 789:	Started container percona-xtradb-cluster-operator
17:14:22 Pod percona-xtradb-cluster-operator-6db54dc756-vdz5v 789:	Stopping container percona-xtradb-cluster-operator
17:14:11 ReplicaSet percona-xtradb-cluster-operator-6db54dc756 783:	Created pod: percona-xtradb-cluster-operator-6db54dc756-vdz5v
17:14:22 ReplicaSet percona-xtradb-cluster-operator-6db54dc756 849:	Deleted pod: percona-xtradb-cluster-operator-6db54dc756-vdz5v
17:14:11 Deployment percona-xtradb-cluster-operator 782:	Scaled up replica set percona-xtradb-cluster-operator-6db54dc756 to 1
17:14:22 Deployment percona-xtradb-cluster-operator 841:	Scaled up replica set percona-xtradb-cluster-operator-64c4fd5974 to 1
17:14:22 Deployment percona-xtradb-cluster-operator 841:	Scaled down replica set percona-xtradb-cluster-operator-6db54dc756 to 0
17:20:27 Pod test-cluster-haproxy-0 1809:	Successfully assigned acto-namespace/test-cluster-haproxy-0 to acto-cluster-4-worker
17:20:28 Pod test-cluster-haproxy-0 1814:	Container image "percona/percona-xtradb-cluster-operator:1.11.0-haproxy" already present on machine
17:20:28 Pod test-cluster-haproxy-0 1814:	Created container haproxy
17:20:28 Pod test-cluster-haproxy-0 1814:	Started container haproxy
17:20:28 Pod test-cluster-haproxy-0 1814:	Container image "percona/percona-xtradb-cluster-operator:1.11.0-haproxy" already present on machine
17:20:28 Pod test-cluster-haproxy-0 1814:	Created container pxc-monit
17:20:28 Pod test-cluster-haproxy-0 1814:	Started container pxc-monit
17:20:47 Pod test-cluster-haproxy-0 1814:	Readiness probe failed: ERROR 2013 (HY000): Lost connection to MySQL server at 'reading initial communication packet', system error: 2

17:21:57 Pod test-cluster-haproxy-0 1814:	Liveness probe failed: ERROR 2013 (HY000): Lost connection to MySQL server at 'reading initial communication packet', system error: 2

17:22:07 Pod test-cluster-haproxy-1 2245:	Successfully assigned acto-namespace/test-cluster-haproxy-1 to acto-cluster-4-worker2
17:22:08 Pod test-cluster-haproxy-1 2247:	Container image "percona/percona-xtradb-cluster-operator:1.11.0-haproxy" already present on machine
17:22:08 Pod test-cluster-haproxy-1 2247:	Created container haproxy
17:22:08 Pod test-cluster-haproxy-1 2247:	Started container haproxy
17:22:08 Pod test-cluster-haproxy-1 2247:	Container image "percona/percona-xtradb-cluster-operator:1.11.0-haproxy" already present on machine
17:22:08 Pod test-cluster-haproxy-1 2247:	Created container pxc-monit
17:22:09 Pod test-cluster-haproxy-1 2247:	Started container pxc-monit
17:30:09 Pod test-cluster-haproxy-1 2247:	Readiness probe failed: 
17:22:28 Pod test-cluster-haproxy-2 2330:	Successfully assigned acto-namespace/test-cluster-haproxy-2 to acto-cluster-4-worker3
17:22:28 Pod test-cluster-haproxy-2 2331:	Container image "percona/percona-xtradb-cluster-operator:1.11.0-haproxy" already present on machine
17:22:28 Pod test-cluster-haproxy-2 2331:	Created container haproxy
17:22:28 Pod test-cluster-haproxy-2 2331:	Started container haproxy
17:22:28 Pod test-cluster-haproxy-2 2331:	Container image "percona/percona-xtradb-cluster-operator:1.11.0-haproxy" already present on machine
17:22:28 Pod test-cluster-haproxy-2 2331:	Created container pxc-monit
17:22:29 Pod test-cluster-haproxy-2 2331:	Started container pxc-monit
17:35:22 Pod test-cluster-haproxy-2 2331:	Stopping container haproxy
17:35:22 Pod test-cluster-haproxy-2 2331:	Stopping container pxc-monit
17:35:23 Pod test-cluster-haproxy-2 2331:	Readiness probe errored: rpc error: code = Unknown desc = failed to exec in container: container is in CONTAINER_EXITED state
17:45:25 Pod test-cluster-haproxy-2 6474:	Successfully assigned acto-namespace/test-cluster-haproxy-2 to acto-cluster-4-worker3
17:45:26 Pod test-cluster-haproxy-2 6476:	Container image "percona/percona-xtradb-cluster-operator:1.11.0-haproxy" already present on machine
17:45:26 Pod test-cluster-haproxy-2 6476:	Created container haproxy
17:45:26 Pod test-cluster-haproxy-2 6476:	Started container haproxy
17:45:26 Pod test-cluster-haproxy-2 6476:	Container image "percona/percona-xtradb-cluster-operator:1.11.0-haproxy" already present on machine
17:45:26 Pod test-cluster-haproxy-2 6476:	Created container pxc-monit
17:45:26 Pod test-cluster-haproxy-2 6476:	Started container pxc-monit
17:20:27 StatefulSet test-cluster-haproxy 1798:	create Pod test-cluster-haproxy-0 in StatefulSet test-cluster-haproxy successful
17:20:27 PodDisruptionBudget test-cluster-haproxy 1807:	No matching pods found
17:22:07 StatefulSet test-cluster-haproxy 1838:	create Pod test-cluster-haproxy-1 in StatefulSet test-cluster-haproxy successful
17:22:28 StatefulSet test-cluster-haproxy 2248:	create Pod test-cluster-haproxy-2 in StatefulSet test-cluster-haproxy successful
17:35:22 StatefulSet test-cluster-haproxy 4745:	delete Pod test-cluster-haproxy-2 in StatefulSet test-cluster-haproxy successful
17:35:23 StatefulSet test-cluster-haproxy 4770:	create Pod test-cluster-haproxy-2 in StatefulSet test-cluster-haproxy failed error: Pod "test-cluster-haproxy-2" is invalid: [spec.containers[2].name: Invalid value: "ACTOKEY": a lowercase RFC 1123 label must consist of lower case alphanumeric characters or '-', and must start and end with an alphanumeric character (e.g. 'my-name',  or '123-abc', regex used for validation is '[a-z0-9]([-a-z0-9]*[a-z0-9])?'), spec.containers[2].lifecycle.postStart.httpGet: Forbidden: may not specify more than 1 handler type, spec.containers[2].livenessProbe.httpGet.scheme: Unsupported value: "ACTOKEY": supported values: "HTTP", "HTTPS", spec.containers[2].livenessProbe.tcpSocket: Forbidden: may not specify more than 1 handler type, spec.containers[2].readinessProbe.terminationGracePeriodSeconds: Invalid value: 1: must not be set for readinessProbes, spec.containers[2].startupProbe.httpGet: Forbidden: may not specify more than 1 handler type, spec.containers[2].startupProbe.successThreshold: Invalid value: 4: must be 1, spec.containers[2].terminationMessagePolicy: Invalid value: "ACTOKEY": must be 'File' or 'FallbackToLogsOnError', spec.containers[2].readinessProbe.httpGet: Forbidden: may not specify more than 1 handler type, spec.containers[2].readinessProbe.tcpSocket: Forbidden: may not specify more than 1 handler type, spec.containers[2].ports[0].name: Invalid value: "ACTOKEY": must contain only alpha-numeric characters (a-z, 0-9), and hyphens (-), spec.containers[2].ports[0].name: Invalid value: "ACTOKEY": must contain at least one letter or number (a-z, 0-9), spec.containers[2].ports[0].protocol: Unsupported value: "ACTOKEY": supported values: "SCTP", "TCP", "UDP", spec.containers[2].ports[1].name: Invalid value: "ACTOKEY": must contain only alpha-numeric characters (a-z, 0-9), and hyphens (-), spec.containers[2].ports[1].name: Invalid value: "ACTOKEY": must contain at least one letter or number (a-z, 0-9), spec.containers[2].ports[1].containerPort: Required value, spec.containers[2].ports[1].protocol: Unsupported value: "ACTOKEY": supported values: "SCTP", "TCP", "UDP", spec.containers[2].env[0].valueFrom.fieldRef.fieldPath: Invalid value: "ACTOKEY": error converting fieldPath: unsupported pod version: ACTOKEY, spec.containers[2].env[0].valueFrom.configMapKeyRef.name: Invalid value: "ACTOKEY": a lowercase RFC 1123 subdomain must consist of lower case alphanumeric characters, '-' or '.', and must start and end with an alphanumeric character (e.g. 'example.com', regex used for validation is '[a-z0-9]([-a-z0-9]*[a-z0-9])?(\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*'), spec.containers[2].env[0].valueFrom.secretKeyRef.name: Invalid value: "ACTOKEY": a lowercase RFC 1123 subdomain must consist of lower case alphanumeric characters, '-' or '.', and must start and end with an alphanumeric character (e.g. 'example.com', regex used for validation is '[a-z0-9]([-a-z0-9]*[a-z0-9])?(\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*'), spec.containers[2].env[0].valueFrom: Invalid value: "": may not be specified when `value` is not empty, spec.containers[2].env[1].valueFrom.fieldRef.fieldPath: Invalid value: "ACTOKEY": error converting fieldPath: unsupported pod version: ACTOKEY, spec.containers[2].env[1].valueFrom.resourceFieldRef.resource: Unsupported value: "ACTOKEY": supported values: "limits.cpu", "limits.ephemeral-storage", "limits.memory", "requests.cpu", "requests.ephemeral-storage", "requests.memory", spec.containers[2].env[1].valueFrom.configMapKeyRef.name: Invalid value: "ACTOKEY": a lowercase RFC 1123 subdomain must consist of lower case alphanumeric characters, '-' or '.', and must start and end with an alphanumeric character (e.g. 'example.com', regex used for validation is '[a-z0-9]([-a-z0-9]*[a-z0-9])?(\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*'), spec.containers[2].env[1].valueFrom.secretKeyRef.name: Invalid value: "ACTOKEY": a lowercase RFC 1123 subdomain must consist of lower case alphanumeric characters, '-' or '.', and must start and end with an alphanumeric character (e.g. 'example.com', regex used for validation is '[a-z0-9]([-a-z0-9]*[a-z0-9])?(\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*'), spec.containers[2].env[1].valueFrom: Invalid value: "": may not be specified when `value` is not empty, spec.containers[2].env[2].valueFrom.fieldRef.fieldPath: Invalid value: "ACTOKEY": error converting fieldPath: unsupported pod version: ACTOKEY, spec.containers[2].env[2].valueFrom.configMapKeyRef.name: Invalid value: "ACTOKEY": a lowercase RFC 1123 subdomain must consist of lower case alphanumeric characters, '-' or '.', and must start and end with an alphanumeric character (e.g. 'example.com', regex used for validation is '[a-z0-9]([-a-z0-9]*[a-z0-9])?(\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*'), spec.containers[2].env[2].valueFrom.secretKeyRef.name: Invalid value: "": a lowercase RFC 1123 subdomain must consist of lower case alphanumeric characters, '-' or '.', and must start and end with an alphanumeric character (e.g. 'example.com', regex used for validation is '[a-z0-9]([-a-z0-9]*[a-z0-9])?(\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*'), spec.containers[2].env[2].valueFrom: Invalid value: "": may not be specified when `value` is not empty, spec.containers[2].envFrom[0].configMapRef.name: Invalid value: "ACTOKEY": a lowercase RFC 1123 subdomain must consist of lower case alphanumeric characters, '-' or '.', and must start and end with an alphanumeric character (e.g. 'example.com', regex used for validation is '[a-z0-9]([-a-z0-9]*[a-z0-9])?(\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*'), spec.containers[2].envFrom[0].secretRef.name: Invalid value: "ACTOKEY": a lowercase RFC 1123 subdomain must consist of lower case alphanumeric characters, '-' or '.', and must start and end with an alphanumeric character (e.g. 'example.com', regex used for validation is '[a-z0-9]([-a-z0-9]*[a-z0-9])?(\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*'), spec.containers[2].envFrom: Invalid value: "": may not have more than one field specified at a time, spec.containers[2].envFrom[1].configMapRef.name: Invalid value: "ACTOKEY": a lowercase RFC 1123 subdomain must consist of lower case alphanumeric characters, '-' or '.', and must start and end with an alphanumeric character (e.g. 'example.com', regex used for validation is '[a-z0-9]([-a-z0-9]*[a-z0-9])?(\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*'), spec.containers[2].envFrom[1].secretRef.name: Invalid value: "ACTOKEY": a lowercase RFC 1123 subdomain must consist of lower case alphanumeric characters, '-' or '.', and must start and end with an alphanumeric character (e.g. 'example.com', regex used for validation is '[a-z0-9]([-a-z0-9]*[a-z0-9])?(\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*'), spec.containers[2].envFrom[2].configMapRef.name: Invalid value: "ACTOKEY": a lowercase RFC 1123 subdomain must consist of lower case alphanumeric characters, '-' or '.', and must start and end with an alphanumeric character (e.g. 'example.com', regex used for validation is '[a-z0-9]([-a-z0-9]*[a-z0-9])?(\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*'), spec.containers[2].envFrom[2].secretRef.name: Invalid value: "ACTOKEY": a lowercase RFC 1123 subdomain must consist of lower case alphanumeric characters, '-' or '.', and must start and end with an alphanumeric character (e.g. 'example.com', regex used for validation is '[a-z0-9]([-a-z0-9]*[a-z0-9])?(\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*'), spec.containers[2].envFrom[3].configMapRef.name: Invalid value: "ACTOKEY": a lowercase RFC 1123 subdomain must consist of lower case alphanumeric characters, '-' or '.', and must start and end with an alphanumeric character (e.g. 'example.com', regex used for validation is '[a-z0-9]([-a-z0-9]*[a-z0-9])?(\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*'), spec.containers[2].envFrom[4].configMapRef.name: Invalid value: "ACTOKEY": a lowercase RFC 1123 subdomain must consist of lower case alphanumeric characters, '-' or '.', and must start and end with an alphanumeric character (e.g. 'example.com', regex used for validation is '[a-z0-9]([-a-z0-9]*[a-z0-9])?(\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*'), spec.containers[2].envFrom[4].secretRef.name: Invalid value: "ACTOKEY": a lowercase RFC 1123 subdomain must consist of lower case alphanumeric characters, '-' or '.', and must start and end with an alphanumeric character (e.g. 'example.com', regex used for validation is '[a-z0-9]([-a-z0-9]*[a-z0-9])?(\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*'), spec.containers[2].volumeMounts[0].name: Not found: "ACTOKEY", spec.containers[2].volumeMounts[0].name: Invalid value: "ACTOKEY": must not already exist in volumeDevices, spec.containers[2].volumeMounts[0].mountPath: Invalid value: "ACTOKEY": must not already exist as a path in volumeDevices, spec.containers[2].volumeMounts[0].subPathExpr: Invalid value: "ACTOKEY": subPathExpr and subPath are mutually exclusive, spec.containers[2].volumeMounts.mountPropagation: Unsupported value: "ACTOKEY": supported values: "Bidirectional", "HostToContainer", "None", spec.containers[2].volumeMounts[1].name: Not found: "ACTOKEY", spec.containers[2].volumeMounts[1].mountPath: Invalid value: "ACTOKEY": must be unique, spec.containers[2].volumeMounts[1].name: Invalid value: "ACTOKEY": must not already exist in volumeDevices, spec.containers[2].volumeMounts[1].mountPath: Invalid value: "ACTOKEY": must not already exist as a path in volumeDevices, spec.containers[2].volumeMounts[1].subPathExpr: Invalid value: "ACTOKEY": subPathExpr and subPath are mutually exclusive, spec.containers[2].volumeMounts[2].name: Not found: "ACTOKEY", spec.containers[2].volumeMounts[2].mountPath: Invalid value: "ACTOKEY": must be unique, spec.containers[2].volumeMounts[2].name: Invalid value: "ACTOKEY": must not already exist in volumeDevices, spec.containers[2].volumeMounts[2].mountPath: Invalid value: "ACTOKEY": must not already exist as a path in volumeDevices, spec.containers[2].volumeMounts[2].subPathExpr: Invalid value: "ACTOKEY": subPathExpr and subPath are mutually exclusive, spec.containers[2].volumeMounts[3].name: Not found: "ACTOKEY", spec.containers[2].volumeMounts[3].mountPath: Invalid value: "ACTOKEY": must be unique, spec.containers[2].volumeMounts[3].name: Invalid value: "ACTOKEY": must not already exist in volumeDevices, spec.containers[2].volumeMounts[3].mountPath: Invalid value: "ACTOKEY": must not already exist as a path in volumeDevices, spec.containers[2].volumeMounts[3].subPathExpr: Invalid value: "ACTOKEY": subPathExpr and subPath are mutually exclusive, spec.containers[2].volumeDevices[0].name: Not found: "ACTOKEY", spec.containers[2].volumeDevices[0].name: Invalid value: "ACTOKEY": must not already exist in volumeMounts, spec.containers[2].volumeDevices[0].devicePath: Invalid value: "ACTOKEY": must not already exist as a path in volumeMounts, spec.containers[2].volumeDevices[1].name: Invalid value: "ACTOKEY": must be unique, spec.containers[2].volumeDevices[1].name: Not found: "ACTOKEY", spec.containers[2].volumeDevices[1].devicePath: Invalid value: "ACTOKEY": must be unique, spec.containers[2].volumeDevices[1].name: Invalid value: "ACTOKEY": must not already exist in volumeMounts, spec.containers[2].volumeDevices[1].devicePath: Invalid value: "ACTOKEY": must not already exist as a path in volumeMounts, spec.containers[2].volumeDevices[2].name: Invalid value: "ACTOKEY": must be unique, spec.containers[2].volumeDevices[2].name: Not found: "ACTOKEY", spec.containers[2].volumeDevices[2].devicePath: Invalid value: "ACTOKEY": must be unique, spec.containers[2].volumeDevices[2].name: Invalid value: "ACTOKEY": must not already exist in volumeMounts, spec.containers[2].volumeDevices[2].devicePath: Invalid value: "ACTOKEY": must not already exist as a path in volumeMounts, spec.containers[2].volumeDevices[3].name: Invalid value: "ACTOKEY": must be unique, spec.containers[2].volumeDevices[3].name: Not found: "ACTOKEY", spec.containers[2].volumeDevices[3].devicePath: Invalid value: "ACTOKEY": must be unique, spec.containers[2].volumeDevices[3].name: Invalid value: "ACTOKEY": must not already exist in volumeMounts, spec.containers[2].volumeDevices[3].devicePath: Invalid value: "ACTOKEY": must not already exist as a path in volumeMounts, spec.containers[2].volumeDevices[4].name: Invalid value: "ACTOKEY": must be unique, spec.containers[2].volumeDevices[4].name: Not found: "ACTOKEY", spec.containers[2].volumeDevices[4].devicePath: Invalid value: "ACTOKEY": must be unique, spec.containers[2].volumeDevices[4].name: Invalid value: "ACTOKEY": must not already exist in volumeMounts, spec.containers[2].volumeDevices[4].devicePath: Invalid value: "ACTOKEY": must not already exist as a path in volumeMounts, spec.containers[2].imagePullPolicy: Unsupported value: "ACTOKEY": supported values: "Always", "IfNotPresent", "Never"]
17:20:33 Pod test-cluster-pxc-0 1780:	Successfully assigned acto-namespace/test-cluster-pxc-0 to acto-cluster-4-worker3
17:20:34 Pod test-cluster-pxc-0 1880:	Container image "percona/percona-xtradb-cluster-operator:1.11.0" already present on machine
17:20:34 Pod test-cluster-pxc-0 1880:	Created container pxc-init
17:20:34 Pod test-cluster-pxc-0 1880:	Started container pxc-init
17:20:35 Pod test-cluster-pxc-0 1880:	Container image "percona/percona-xtradb-cluster-operator:1.11.0-logcollector" already present on machine
17:20:35 Pod test-cluster-pxc-0 1880:	Created container logs
17:20:35 Pod test-cluster-pxc-0 1880:	Started container logs
17:20:35 Pod test-cluster-pxc-0 1880:	Container image "percona/percona-xtradb-cluster-operator:1.11.0-logcollector" already present on machine
17:20:35 Pod test-cluster-pxc-0 1880:	Created container logrotate
17:20:36 Pod test-cluster-pxc-0 1880:	Started container logrotate
17:20:36 Pod test-cluster-pxc-0 1880:	Container image "percona/percona-xtradb-cluster:8.0.27-18.1" already present on machine
17:20:36 Pod test-cluster-pxc-0 1880:	Created container pxc
17:20:36 Pod test-cluster-pxc-0 1880:	Started container pxc
17:21:03 Pod test-cluster-pxc-0 1880:	Readiness probe failed: ERROR 2003 (HY000): Can't connect to MySQL server on '10.244.1.3:33062' (111)
+ [[ '' == \P\r\i\m\a\r\y ]]
+ exit 1

17:29:58 Pod test-cluster-pxc-0 1880:	Stopping container logs
17:29:58 Pod test-cluster-pxc-0 1880:	Stopping container pxc
17:29:58 Pod test-cluster-pxc-0 1880:	Stopping container logrotate
17:30:12 Pod test-cluster-pxc-0 3829:	Successfully assigned acto-namespace/test-cluster-pxc-0 to acto-cluster-4-worker3
17:30:14 Pod test-cluster-pxc-0 3833:	Container image "percona/percona-xtradb-cluster-operator:1.11.0" already present on machine
17:30:14 Pod test-cluster-pxc-0 3833:	Created container pxc-init
17:30:14 Pod test-cluster-pxc-0 3833:	Started container pxc-init
17:30:15 Pod test-cluster-pxc-0 3833:	Container image "percona/percona-xtradb-cluster-operator:1.11.0-logcollector" already present on machine
17:30:15 Pod test-cluster-pxc-0 3833:	Created container logs
17:30:15 Pod test-cluster-pxc-0 3833:	Started container logs
17:30:15 Pod test-cluster-pxc-0 3833:	Container image "percona/percona-xtradb-cluster-operator:1.11.0-logcollector" already present on machine
17:30:15 Pod test-cluster-pxc-0 3833:	Created container logrotate
17:30:16 Pod test-cluster-pxc-0 3833:	Started container logrotate
17:30:16 Pod test-cluster-pxc-0 3833:	Container image "percona/percona-xtradb-cluster:8.0.27-18.1" already present on machine
17:30:16 Pod test-cluster-pxc-0 3833:	Created container pxc
17:30:16 Pod test-cluster-pxc-0 3833:	Started container pxc
17:21:38 Pod test-cluster-pxc-1 2088:	Successfully assigned acto-namespace/test-cluster-pxc-1 to acto-cluster-4-worker2
17:21:39 Pod test-cluster-pxc-1 2131:	Container image "percona/percona-xtradb-cluster-operator:1.11.0" already present on machine
17:21:39 Pod test-cluster-pxc-1 2131:	Created container pxc-init
17:21:39 Pod test-cluster-pxc-1 2131:	Started container pxc-init
17:21:40 Pod test-cluster-pxc-1 2131:	Container image "percona/percona-xtradb-cluster-operator:1.11.0-logcollector" already present on machine
17:21:40 Pod test-cluster-pxc-1 2131:	Created container logs
17:21:40 Pod test-cluster-pxc-1 2131:	Started container logs
17:21:40 Pod test-cluster-pxc-1 2131:	Container image "percona/percona-xtradb-cluster-operator:1.11.0-logcollector" already present on machine
17:21:40 Pod test-cluster-pxc-1 2131:	Created container logrotate
17:21:41 Pod test-cluster-pxc-1 2131:	Started container logrotate
17:21:41 Pod test-cluster-pxc-1 2131:	Container image "percona/percona-xtradb-cluster:8.0.27-18.1" already present on machine
17:21:41 Pod test-cluster-pxc-1 2131:	Created container pxc
17:21:41 Pod test-cluster-pxc-1 2131:	Started container pxc
17:22:08 Pod test-cluster-pxc-1 2131:	Readiness probe failed: ERROR 2003 (HY000): Can't connect to MySQL server on '10.244.3.4:33062' (111)
+ [[ '' == \P\r\i\m\a\r\y ]]
+ exit 1

17:29:07 Pod test-cluster-pxc-1 2131:	Stopping container logs
17:29:07 Pod test-cluster-pxc-1 2131:	Stopping container pxc
17:29:07 Pod test-cluster-pxc-1 2131:	Stopping container logrotate
17:29:22 Pod test-cluster-pxc-1 3646:	Successfully assigned acto-namespace/test-cluster-pxc-1 to acto-cluster-4-worker2
17:29:23 Pod test-cluster-pxc-1 3647:	Container image "percona/percona-xtradb-cluster-operator:1.11.0" already present on machine
17:29:23 Pod test-cluster-pxc-1 3647:	Created container pxc-init
17:29:23 Pod test-cluster-pxc-1 3647:	Started container pxc-init
17:29:25 Pod test-cluster-pxc-1 3647:	Container image "percona/percona-xtradb-cluster-operator:1.11.0-logcollector" already present on machine
17:29:25 Pod test-cluster-pxc-1 3647:	Created container logs
17:29:25 Pod test-cluster-pxc-1 3647:	Started container logs
17:29:25 Pod test-cluster-pxc-1 3647:	Container image "percona/percona-xtradb-cluster-operator:1.11.0-logcollector" already present on machine
17:29:25 Pod test-cluster-pxc-1 3647:	Created container logrotate
17:29:25 Pod test-cluster-pxc-1 3647:	Started container logrotate
17:29:25 Pod test-cluster-pxc-1 3647:	Container image "percona/percona-xtradb-cluster:8.0.27-18.1" already present on machine
17:29:25 Pod test-cluster-pxc-1 3647:	Created container pxc
17:29:25 Pod test-cluster-pxc-1 3647:	Started container pxc
17:22:45 Pod test-cluster-pxc-2 2390:	Successfully assigned acto-namespace/test-cluster-pxc-2 to acto-cluster-4-worker
17:22:45 Pod test-cluster-pxc-2 2436:	Container image "percona/percona-xtradb-cluster-operator:1.11.0" already present on machine
17:22:45 Pod test-cluster-pxc-2 2436:	Created container pxc-init
17:22:46 Pod test-cluster-pxc-2 2436:	Started container pxc-init
17:22:47 Pod test-cluster-pxc-2 2436:	Container image "percona/percona-xtradb-cluster-operator:1.11.0-logcollector" already present on machine
17:22:47 Pod test-cluster-pxc-2 2436:	Created container logs
17:22:47 Pod test-cluster-pxc-2 2436:	Started container logs
17:22:47 Pod test-cluster-pxc-2 2436:	Container image "percona/percona-xtradb-cluster-operator:1.11.0-logcollector" already present on machine
17:22:47 Pod test-cluster-pxc-2 2436:	Created container logrotate
17:22:47 Pod test-cluster-pxc-2 2436:	Started container logrotate
17:22:47 Pod test-cluster-pxc-2 2436:	Container image "percona/percona-xtradb-cluster:8.0.27-18.1" already present on machine
17:22:47 Pod test-cluster-pxc-2 2436:	Created container pxc
17:22:48 Pod test-cluster-pxc-2 2436:	Started container pxc
17:23:15 Pod test-cluster-pxc-2 2436:	Readiness probe failed: ERROR 2003 (HY000): Can't connect to MySQL server on '10.244.2.5:33062' (111)
+ [[ '' == \P\r\i\m\a\r\y ]]
+ exit 1

17:28:17 Pod test-cluster-pxc-2 2436:	Stopping container logs
17:28:17 Pod test-cluster-pxc-2 2436:	Stopping container pxc
17:28:17 Pod test-cluster-pxc-2 2436:	Stopping container logrotate
17:28:31 Pod test-cluster-pxc-2 3458:	Successfully assigned acto-namespace/test-cluster-pxc-2 to acto-cluster-4-worker
17:28:33 Pod test-cluster-pxc-2 3459:	Container image "percona/percona-xtradb-cluster-operator:1.11.0" already present on machine
17:28:33 Pod test-cluster-pxc-2 3459:	Created container pxc-init
17:28:33 Pod test-cluster-pxc-2 3459:	Started container pxc-init
17:28:34 Pod test-cluster-pxc-2 3459:	Container image "percona/percona-xtradb-cluster-operator:1.11.0-logcollector" already present on machine
17:28:34 Pod test-cluster-pxc-2 3459:	Created container logs
17:28:34 Pod test-cluster-pxc-2 3459:	Started container logs
17:28:34 Pod test-cluster-pxc-2 3459:	Container image "percona/percona-xtradb-cluster-operator:1.11.0-logcollector" already present on machine
17:28:34 Pod test-cluster-pxc-2 3459:	Created container logrotate
17:28:34 Pod test-cluster-pxc-2 3459:	Started container logrotate
17:28:34 Pod test-cluster-pxc-2 3459:	Container image "percona/percona-xtradb-cluster:8.0.27-18.1" already present on machine
17:28:35 Pod test-cluster-pxc-2 3459:	Created container pxc
17:28:35 Pod test-cluster-pxc-2 3459:	Started container pxc
17:20:27 StatefulSet test-cluster-pxc 1776:	create Claim datadir-test-cluster-pxc-0 Pod test-cluster-pxc-0 in StatefulSet test-cluster-pxc success
17:20:27 StatefulSet test-cluster-pxc 1776:	create Pod test-cluster-pxc-0 in StatefulSet test-cluster-pxc successful
17:21:33 StatefulSet test-cluster-pxc 2068:	create Claim datadir-test-cluster-pxc-1 Pod test-cluster-pxc-1 in StatefulSet test-cluster-pxc success
17:21:33 StatefulSet test-cluster-pxc 2068:	create Pod test-cluster-pxc-1 in StatefulSet test-cluster-pxc successful
17:22:39 StatefulSet test-cluster-pxc 2380:	create Claim datadir-test-cluster-pxc-2 Pod test-cluster-pxc-2 in StatefulSet test-cluster-pxc success
17:22:39 StatefulSet test-cluster-pxc 2380:	create Pod test-cluster-pxc-2 in StatefulSet test-cluster-pxc successful
