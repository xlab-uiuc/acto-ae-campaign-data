04:35:41 Pod acto-test-operator-zookeeper-operator-658b9fbcfb-dswfg 722:	Successfully assigned acto-namespace/acto-test-operator-zookeeper-operator-658b9fbcfb-dswfg to acto-cluster-8-worker2
04:35:41 Pod acto-test-operator-zookeeper-operator-658b9fbcfb-dswfg 726:	Container image "pravega/zookeeper-operator:0.2.13" already present on machine
04:35:41 Pod acto-test-operator-zookeeper-operator-658b9fbcfb-dswfg 726:	Created container acto-test-operator-zookeeper-operator
04:35:41 Pod acto-test-operator-zookeeper-operator-658b9fbcfb-dswfg 726:	Started container acto-test-operator-zookeeper-operator
04:35:41 ReplicaSet acto-test-operator-zookeeper-operator-658b9fbcfb 718:	Created pod: acto-test-operator-zookeeper-operator-658b9fbcfb-dswfg
04:35:41 Deployment acto-test-operator-zookeeper-operator 717:	Scaled up replica set acto-test-operator-zookeeper-operator-658b9fbcfb to 1
04:36:14 PersistentVolumeClaim data-test-cluster-0 817:	waiting for first consumer to be created before binding
04:36:14 PersistentVolumeClaim data-test-cluster-0 830:	waiting for a volume to be created, either by external provisioner "rancher.io/local-path" or manually created by system administrator
04:36:14 PersistentVolumeClaim data-test-cluster-0 830:	External provisioner is provisioning volume for claim "acto-namespace/data-test-cluster-0"
04:36:18 PersistentVolumeClaim data-test-cluster-0 830:	Successfully provisioned volume pvc-c67fa504-74ba-4cdf-b8a2-616673696cc0
04:36:39 PersistentVolumeClaim data-test-cluster-1 909:	waiting for first consumer to be created before binding
04:36:39 PersistentVolumeClaim data-test-cluster-1 918:	waiting for a volume to be created, either by external provisioner "rancher.io/local-path" or manually created by system administrator
04:36:39 PersistentVolumeClaim data-test-cluster-1 918:	External provisioner is provisioning volume for claim "acto-namespace/data-test-cluster-1"
04:36:43 PersistentVolumeClaim data-test-cluster-1 918:	Successfully provisioned volume pvc-62f89041-1f9f-426e-aa8d-c2f3c64c81ff
04:37:07 PersistentVolumeClaim data-test-cluster-2 1006:	waiting for first consumer to be created before binding
04:37:07 PersistentVolumeClaim data-test-cluster-2 1015:	waiting for a volume to be created, either by external provisioner "rancher.io/local-path" or manually created by system administrator
04:37:07 PersistentVolumeClaim data-test-cluster-2 1015:	External provisioner is provisioning volume for claim "acto-namespace/data-test-cluster-2"
04:37:11 PersistentVolumeClaim data-test-cluster-2 1015:	Successfully provisioned volume pvc-166aeeae-fdbf-426e-8d71-bb363af93ba5
04:36:19 Pod test-cluster-0 822:	Successfully assigned acto-namespace/test-cluster-0 to acto-cluster-8-worker3
04:36:20 Pod test-cluster-0 860:	Container image "pravega/zookeeper:0.2.14" already present on machine
04:36:20 Pod test-cluster-0 860:	Created container zookeeper
04:36:20 Pod test-cluster-0 860:	Started container zookeeper
04:42:07 Pod test-cluster-0 860:	Stopping container zookeeper
04:42:39 Pod test-cluster-0 1695:	Successfully assigned acto-namespace/test-cluster-0 to acto-cluster-8-worker3
04:42:40 Pod test-cluster-0 1696:	Container image "pravega/zookeeper:0.2.14" already present on machine
04:42:40 Pod test-cluster-0 1696:	Created container zookeeper
04:42:40 Pod test-cluster-0 1696:	Started container zookeeper
04:45:29 Pod test-cluster-0 1696:	Stopping container zookeeper
04:46:00 Pod test-cluster-0 2165:	Successfully assigned acto-namespace/test-cluster-0 to acto-cluster-8-worker3
04:46:01 Pod test-cluster-0 2166:	Container image "pravega/zookeeper:0.2.14" already present on machine
04:46:01 Pod test-cluster-0 2166:	Created container zookeeper
04:46:01 Pod test-cluster-0 2166:	Started container zookeeper
04:48:51 Pod test-cluster-0 2166:	Stopping container zookeeper
04:49:25 Pod test-cluster-0 2632:	Successfully assigned acto-namespace/test-cluster-0 to acto-cluster-8-worker3
04:49:25 Pod test-cluster-0 2635:	Container image "pravega/zookeeper:0.2.14" already present on machine
04:49:26 Pod test-cluster-0 2635:	Created container zookeeper
04:49:26 Pod test-cluster-0 2635:	Started container zookeeper
04:52:14 Pod test-cluster-0 2635:	Stopping container zookeeper
04:52:46 Pod test-cluster-0 3098:	Successfully assigned acto-namespace/test-cluster-0 to acto-cluster-8-worker3
04:52:47 Pod test-cluster-0 3099:	Container image "pravega/zookeeper:0.2.14" already present on machine
04:52:47 Pod test-cluster-0 3099:	Created container zookeeper
04:52:47 Pod test-cluster-0 3099:	Started container zookeeper
04:54:38 Pod test-cluster-0 3099:	Stopping container zookeeper
04:54:40 Pod test-cluster-0 3099:	Exec lifecycle hook ([zookeeperTeardown.sh]) for Container "zookeeper" in Pod "test-cluster-0_acto-namespace(bf6fadb7-0fd2-4678-9876-d03593149595)" failed - error: command 'zookeeperTeardown.sh' exited with 137: + source /conf/env.sh
++ DOMAIN=test-cluster-headless.acto-namespace.svc.cluster.local
++ QUORUM_PORT=2888
++ LEADER_PORT=3888
++ CLIENT_HOST=test-cluster-client
++ CLIENT_PORT=2181
++ ADMIN_SERVER_HOST=test-cluster-admin-server
++ ADMIN_SERVER_PORT=8080
++ CLUSTER_NAME=test-cluster
++ CLUSTER_SIZE=3
+ source /usr/local/bin/zookeeperFunctions.sh
++ set -ex
+ DATA_DIR=/data
+ MYID_FILE=/data/myid
+ LOG4J_CONF=/conf/log4j-quiet.properties
++ echo cons
++ nc localhost 2181
++ grep -v '^$'
++ grep -v /127.0.0.1:
++ wc -l
+ CONN_COUNT=1
+ (( i = 0 ))
+ (( i < 6 ))
+ [[ 1 -gt 0 ]]
+ echo '1 non-local connections still connected.'
+ sleep 5
, message: "1 non-local connections still connected.\n+ source /conf/env.sh\n++ DOMAIN=test-cluster-headless.acto-namespace.svc.cluster.local\n++ QUORUM_PORT=2888\n++ LEADER_PORT=3888\n++ CLIENT_HOST=test-cluster-client\n++ CLIENT_PORT=2181\n++ ADMIN_SERVER_HOST=test-cluster-admin-server\n++ ADMIN_SERVER_PORT=8080\n++ CLUSTER_NAME=test-cluster\n++ CLUSTER_SIZE=3\n+ source /usr/local/bin/zookeeperFunctions.sh\n++ set -ex\n+ DATA_DIR=/data\n+ MYID_FILE=/data/myid\n+ LOG4J_CONF=/conf/log4j-quiet.properties\n++ echo cons\n++ nc localhost 2181\n++ grep -v '^$'\n++ grep -v /127.0.0.1:\n++ wc -l\n+ CONN_COUNT=1\n+ (( i = 0 ))\n+ (( i < 6 ))\n+ [[ 1 -gt 0 ]]\n+ echo '1 non-local connections still connected.'\n+ sleep 5\n"
04:54:40 Pod test-cluster-0 3423:	Successfully assigned acto-namespace/test-cluster-0 to acto-cluster-8-worker3
04:54:41 Pod test-cluster-0 3424:	Container image "pravega/zookeeper:0.2.14" already present on machine
04:54:41 Pod test-cluster-0 3424:	Created container zookeeper
04:54:42 Pod test-cluster-0 3424:	Started container zookeeper
04:58:32 Pod test-cluster-0 3424:	Stopping container zookeeper
04:59:04 Pod test-cluster-0 3976:	Successfully assigned acto-namespace/test-cluster-0 to acto-cluster-8-worker3
04:59:05 Pod test-cluster-0 3977:	Container image "pravega/zookeeper:0.2.14" already present on machine
04:59:05 Pod test-cluster-0 3977:	Created container zookeeper
04:59:05 Pod test-cluster-0 3977:	Started container zookeeper
05:01:14 Pod test-cluster-0 3977:	Stopping container zookeeper
05:01:25 Pod test-cluster-0 3977:	Exec lifecycle hook ([zookeeperTeardown.sh]) for Container "zookeeper" in Pod "test-cluster-0_acto-namespace(bcad10ee-245f-42a8-86e8-7e958a8b342c)" failed - error: command 'zookeeperTeardown.sh' exited with 137: + source /conf/env.sh
++ DOMAIN=test-cluster-headless.acto-namespace.svc.cluster.local
++ QUORUM_PORT=2888
++ LEADER_PORT=3888
++ CLIENT_HOST=test-cluster-client
++ CLIENT_PORT=2181
++ ADMIN_SERVER_HOST=test-cluster-admin-server
++ ADMIN_SERVER_PORT=8080
++ CLUSTER_NAME=test-cluster
++ CLUSTER_SIZE=3
+ source /usr/local/bin/zookeeperFunctions.sh
++ set -ex
+ DATA_DIR=/data
+ MYID_FILE=/data/myid
+ LOG4J_CONF=/conf/log4j-quiet.properties
++ echo cons
++ nc localhost 2181
++ grep -v '^$'
++ grep -v /127.0.0.1:
++ wc -l
+ CONN_COUNT=1
+ (( i = 0 ))
+ (( i < 6 ))
+ [[ 1 -gt 0 ]]
+ echo '1 non-local connections still connected.'
+ sleep 5
+ (( i++  ))
+ (( i < 6 ))
+ [[ 1 -gt 0 ]]
+ echo '1 non-local connections still connected.'
+ sleep 5
, message: "1 non-local connections still connected.\n1 non-local connections still connected.\n+ source /conf/env.sh\n++ DOMAIN=test-cluster-headless.acto-namespace.svc.cluster.local\n++ QUORUM_PORT=2888\n++ LEADER_PORT=3888\n++ CLIENT_HOST=test-cluster-client\n++ CLIENT_PORT=2181\n++ ADMIN_SERVER_HOST=test-cluster-admin-server\n++ ADMIN_SERVER_PORT=8080\n++ CLUSTER_NAME=test-cluster\n++ CLUSTER_SIZE=3\n+ source /usr/local/bin/zookeeperFunctions.sh\n++ set -ex\n+ DATA_DIR=/data\n+ MYID_FILE=/data/myid\n+ LOG4J_CONF=/conf/log4j-quiet.properties\n++ echo cons\n++ nc localhost 2181\n++ grep -v '^$'\n++ grep -v /127.0.0.1:\n++ wc -l\n+ CONN_COUNT=1\n+ (( i = 0 ))\n+ (( i < 6 ))\n+ [[ 1 -gt 0 ]]\n+ echo '1 non-local connections still connected.'\n+ sleep 5\n+ (( i++  ))\n+ (( i < 6 ))\n+ [[ 1 -gt 0 ]]\n+ echo '1 non-local connections still connected.'\n+ sleep 5\n"
05:01:25 Pod test-cluster-0 4346:	Successfully assigned acto-namespace/test-cluster-0 to acto-cluster-8-worker3
05:01:26 Pod test-cluster-0 4348:	Container image "pravega/zookeeper:0.2.14" already present on machine
05:01:26 Pod test-cluster-0 4348:	Created container zookeeper
05:01:26 Pod test-cluster-0 4348:	Started container zookeeper
04:36:44 Pod test-cluster-1 914:	Successfully assigned acto-namespace/test-cluster-1 to acto-cluster-8-worker
04:36:45 Pod test-cluster-1 945:	Container image "pravega/zookeeper:0.2.14" already present on machine
04:36:45 Pod test-cluster-1 945:	Created container zookeeper
04:36:45 Pod test-cluster-1 945:	Started container zookeeper
04:41:14 Pod test-cluster-1 945:	Stopping container zookeeper
04:41:46 Pod test-cluster-1 1566:	Successfully assigned acto-namespace/test-cluster-1 to acto-cluster-8-worker
04:41:47 Pod test-cluster-1 1567:	Container image "pravega/zookeeper:0.2.14" already present on machine
04:41:47 Pod test-cluster-1 1567:	Created container zookeeper
04:41:47 Pod test-cluster-1 1567:	Started container zookeeper
04:44:36 Pod test-cluster-1 1567:	Stopping container zookeeper
04:45:08 Pod test-cluster-1 2031:	Successfully assigned acto-namespace/test-cluster-1 to acto-cluster-8-worker
04:45:09 Pod test-cluster-1 2032:	Container image "pravega/zookeeper:0.2.14" already present on machine
04:45:09 Pod test-cluster-1 2032:	Created container zookeeper
04:45:09 Pod test-cluster-1 2032:	Started container zookeeper
04:47:59 Pod test-cluster-1 2032:	Stopping container zookeeper
04:48:31 Pod test-cluster-1 2502:	Successfully assigned acto-namespace/test-cluster-1 to acto-cluster-8-worker
04:48:31 Pod test-cluster-1 2503:	Container image "pravega/zookeeper:0.2.14" already present on machine
04:48:31 Pod test-cluster-1 2503:	Created container zookeeper
04:48:32 Pod test-cluster-1 2503:	Started container zookeeper
04:51:22 Pod test-cluster-1 2503:	Stopping container zookeeper
04:51:54 Pod test-cluster-1 2968:	Successfully assigned acto-namespace/test-cluster-1 to acto-cluster-8-worker
04:51:55 Pod test-cluster-1 2969:	Container image "pravega/zookeeper:0.2.14" already present on machine
04:51:55 Pod test-cluster-1 2969:	Created container zookeeper
04:51:55 Pod test-cluster-1 2969:	Started container zookeeper
04:54:14 Pod test-cluster-1 2969:	Stopping container zookeeper
04:54:16 Pod test-cluster-1 2969:	Exec lifecycle hook ([zookeeperTeardown.sh]) for Container "zookeeper" in Pod "test-cluster-1_acto-namespace(51cb2c43-1c6d-42f6-b9cb-a8fa35bc933f)" failed - error: command 'zookeeperTeardown.sh' exited with 137: + source /conf/env.sh
++ DOMAIN=test-cluster-headless.acto-namespace.svc.cluster.local
++ QUORUM_PORT=2888
++ LEADER_PORT=3888
++ CLIENT_HOST=test-cluster-client
++ CLIENT_PORT=2181
++ ADMIN_SERVER_HOST=test-cluster-admin-server
++ ADMIN_SERVER_PORT=8080
++ CLUSTER_NAME=test-cluster
++ CLUSTER_SIZE=3
+ source /usr/local/bin/zookeeperFunctions.sh
++ set -ex
+ DATA_DIR=/data
+ MYID_FILE=/data/myid
+ LOG4J_CONF=/conf/log4j-quiet.properties
++ echo cons
++ nc localhost 2181
++ grep -v '^$'
++ grep -v /127.0.0.1:
++ wc -l
+ CONN_COUNT=1
+ (( i = 0 ))
+ (( i < 6 ))
+ [[ 1 -gt 0 ]]
+ echo '1 non-local connections still connected.'
+ sleep 5
, message: "1 non-local connections still connected.\n+ source /conf/env.sh\n++ DOMAIN=test-cluster-headless.acto-namespace.svc.cluster.local\n++ QUORUM_PORT=2888\n++ LEADER_PORT=3888\n++ CLIENT_HOST=test-cluster-client\n++ CLIENT_PORT=2181\n++ ADMIN_SERVER_HOST=test-cluster-admin-server\n++ ADMIN_SERVER_PORT=8080\n++ CLUSTER_NAME=test-cluster\n++ CLUSTER_SIZE=3\n+ source /usr/local/bin/zookeeperFunctions.sh\n++ set -ex\n+ DATA_DIR=/data\n+ MYID_FILE=/data/myid\n+ LOG4J_CONF=/conf/log4j-quiet.properties\n++ echo cons\n++ nc localhost 2181\n++ grep -v '^$'\n++ grep -v /127.0.0.1:\n++ wc -l\n+ CONN_COUNT=1\n+ (( i = 0 ))\n+ (( i < 6 ))\n+ [[ 1 -gt 0 ]]\n+ echo '1 non-local connections still connected.'\n+ sleep 5\n"
04:54:18 Pod test-cluster-1 3339:	Successfully assigned acto-namespace/test-cluster-1 to acto-cluster-8-worker
04:54:18 Pod test-cluster-1 3340:	Container image "pravega/zookeeper:0.2.14" already present on machine
04:54:18 Pod test-cluster-1 3340:	Created container zookeeper
04:54:19 Pod test-cluster-1 3340:	Started container zookeeper
04:57:40 Pod test-cluster-1 3340:	Stopping container zookeeper
04:58:12 Pod test-cluster-1 3848:	Successfully assigned acto-namespace/test-cluster-1 to acto-cluster-8-worker
04:58:13 Pod test-cluster-1 3849:	Container image "pravega/zookeeper:0.2.14" already present on machine
04:58:13 Pod test-cluster-1 3849:	Created container zookeeper
04:58:13 Pod test-cluster-1 3849:	Started container zookeeper
05:00:43 Pod test-cluster-1 3849:	Stopping container zookeeper
05:00:53 Pod test-cluster-1 3849:	Exec lifecycle hook ([zookeeperTeardown.sh]) for Container "zookeeper" in Pod "test-cluster-1_acto-namespace(78603845-d6e5-4d74-b614-702bff626ced)" failed - error: command 'zookeeperTeardown.sh' exited with 137: + source /conf/env.sh
++ DOMAIN=test-cluster-headless.acto-namespace.svc.cluster.local
++ QUORUM_PORT=2888
++ LEADER_PORT=3888
++ CLIENT_HOST=test-cluster-client
++ CLIENT_PORT=2181
++ ADMIN_SERVER_HOST=test-cluster-admin-server
++ ADMIN_SERVER_PORT=8080
++ CLUSTER_NAME=test-cluster
++ CLUSTER_SIZE=3
+ source /usr/local/bin/zookeeperFunctions.sh
++ set -ex
+ DATA_DIR=/data
+ MYID_FILE=/data/myid
+ LOG4J_CONF=/conf/log4j-quiet.properties
++ echo cons
++ nc localhost 2181
++ grep -v '^$'
++ grep -v /127.0.0.1:
++ wc -l
+ CONN_COUNT=1
+ (( i = 0 ))
+ (( i < 6 ))
+ [[ 1 -gt 0 ]]
+ echo '1 non-local connections still connected.'
+ sleep 5
+ (( i++  ))
+ (( i < 6 ))
+ [[ 1 -gt 0 ]]
+ echo '1 non-local connections still connected.'
+ sleep 5
, message: "1 non-local connections still connected.\n1 non-local connections still connected.\n+ source /conf/env.sh\n++ DOMAIN=test-cluster-headless.acto-namespace.svc.cluster.local\n++ QUORUM_PORT=2888\n++ LEADER_PORT=3888\n++ CLIENT_HOST=test-cluster-client\n++ CLIENT_PORT=2181\n++ ADMIN_SERVER_HOST=test-cluster-admin-server\n++ ADMIN_SERVER_PORT=8080\n++ CLUSTER_NAME=test-cluster\n++ CLUSTER_SIZE=3\n+ source /usr/local/bin/zookeeperFunctions.sh\n++ set -ex\n+ DATA_DIR=/data\n+ MYID_FILE=/data/myid\n+ LOG4J_CONF=/conf/log4j-quiet.properties\n++ echo cons\n++ nc localhost 2181\n++ grep -v '^$'\n++ grep -v /127.0.0.1:\n++ wc -l\n+ CONN_COUNT=1\n+ (( i = 0 ))\n+ (( i < 6 ))\n+ [[ 1 -gt 0 ]]\n+ echo '1 non-local connections still connected.'\n+ sleep 5\n+ (( i++  ))\n+ (( i < 6 ))\n+ [[ 1 -gt 0 ]]\n+ echo '1 non-local connections still connected.'\n+ sleep 5\n"
05:00:54 Pod test-cluster-1 4253:	Successfully assigned acto-namespace/test-cluster-1 to acto-cluster-8-worker
05:00:55 Pod test-cluster-1 4254:	Container image "pravega/zookeeper:0.2.14" already present on machine
05:00:55 Pod test-cluster-1 4254:	Created container zookeeper
05:00:55 Pod test-cluster-1 4254:	Started container zookeeper
04:37:12 Pod test-cluster-2 1011:	Successfully assigned acto-namespace/test-cluster-2 to acto-cluster-8-worker2
04:37:13 Pod test-cluster-2 1040:	Container image "pravega/zookeeper:0.2.14" already present on machine
04:37:13 Pod test-cluster-2 1040:	Created container zookeeper
04:37:13 Pod test-cluster-2 1040:	Started container zookeeper
04:40:21 Pod test-cluster-2 1040:	Stopping container zookeeper
04:40:53 Pod test-cluster-2 1440:	Successfully assigned acto-namespace/test-cluster-2 to acto-cluster-8-worker2
04:40:54 Pod test-cluster-2 1441:	Container image "pravega/zookeeper:0.2.14" already present on machine
04:40:54 Pod test-cluster-2 1441:	Created container zookeeper
04:40:54 Pod test-cluster-2 1441:	Started container zookeeper
04:43:44 Pod test-cluster-2 1441:	Stopping container zookeeper
04:44:16 Pod test-cluster-2 1900:	Successfully assigned acto-namespace/test-cluster-2 to acto-cluster-8-worker2
04:44:17 Pod test-cluster-2 1901:	Container image "pravega/zookeeper:0.2.14" already present on machine
04:44:17 Pod test-cluster-2 1901:	Created container zookeeper
04:44:17 Pod test-cluster-2 1901:	Started container zookeeper
04:47:07 Pod test-cluster-2 1901:	Stopping container zookeeper
04:47:38 Pod test-cluster-2 2375:	Successfully assigned acto-namespace/test-cluster-2 to acto-cluster-8-worker2
04:47:39 Pod test-cluster-2 2378:	Container image "pravega/zookeeper:0.2.14" already present on machine
04:47:39 Pod test-cluster-2 2378:	Created container zookeeper
04:47:39 Pod test-cluster-2 2378:	Started container zookeeper
04:50:29 Pod test-cluster-2 2378:	Stopping container zookeeper
04:51:02 Pod test-cluster-2 2843:	Successfully assigned acto-namespace/test-cluster-2 to acto-cluster-8-worker2
04:51:03 Pod test-cluster-2 2844:	Container image "pravega/zookeeper:0.2.14" already present on machine
04:51:03 Pod test-cluster-2 2844:	Created container zookeeper
04:51:03 Pod test-cluster-2 2844:	Started container zookeeper
04:53:51 Pod test-cluster-2 2844:	Stopping container zookeeper
04:53:53 Pod test-cluster-2 2844:	Exec lifecycle hook ([zookeeperTeardown.sh]) for Container "zookeeper" in Pod "test-cluster-2_acto-namespace(7fc7bdbb-79bf-48e2-b8f1-df9513ca4551)" failed - error: command 'zookeeperTeardown.sh' exited with 137: + source /conf/env.sh
++ DOMAIN=test-cluster-headless.acto-namespace.svc.cluster.local
++ QUORUM_PORT=2888
++ LEADER_PORT=3888
++ CLIENT_HOST=test-cluster-client
++ CLIENT_PORT=2181
++ ADMIN_SERVER_HOST=test-cluster-admin-server
++ ADMIN_SERVER_PORT=8080
++ CLUSTER_NAME=test-cluster
++ CLUSTER_SIZE=3
+ source /usr/local/bin/zookeeperFunctions.sh
++ set -ex
+ DATA_DIR=/data
+ MYID_FILE=/data/myid
+ LOG4J_CONF=/conf/log4j-quiet.properties
++ echo cons
++ grep -v '^$'
++ nc localhost 2181
++ grep -v /127.0.0.1:
++ wc -l
+ CONN_COUNT=2
+ (( i = 0 ))
+ (( i < 6 ))
+ [[ 2 -gt 0 ]]
+ echo '2 non-local connections still connected.'
+ sleep 5
, message: "2 non-local connections still connected.\n+ source /conf/env.sh\n++ DOMAIN=test-cluster-headless.acto-namespace.svc.cluster.local\n++ QUORUM_PORT=2888\n++ LEADER_PORT=3888\n++ CLIENT_HOST=test-cluster-client\n++ CLIENT_PORT=2181\n++ ADMIN_SERVER_HOST=test-cluster-admin-server\n++ ADMIN_SERVER_PORT=8080\n++ CLUSTER_NAME=test-cluster\n++ CLUSTER_SIZE=3\n+ source /usr/local/bin/zookeeperFunctions.sh\n++ set -ex\n+ DATA_DIR=/data\n+ MYID_FILE=/data/myid\n+ LOG4J_CONF=/conf/log4j-quiet.properties\n++ echo cons\n++ grep -v '^$'\n++ nc localhost 2181\n++ grep -v /127.0.0.1:\n++ wc -l\n+ CONN_COUNT=2\n+ (( i = 0 ))\n+ (( i < 6 ))\n+ [[ 2 -gt 0 ]]\n+ echo '2 non-local connections still connected.'\n+ sleep 5\n"
04:53:54 Pod test-cluster-2 3257:	Successfully assigned acto-namespace/test-cluster-2 to acto-cluster-8-worker2
04:53:55 Pod test-cluster-2 3258:	Container image "pravega/zookeeper:0.2.14" already present on machine
04:53:55 Pod test-cluster-2 3258:	Created container zookeeper
04:53:55 Pod test-cluster-2 3258:	Started container zookeeper
04:56:48 Pod test-cluster-2 3258:	Stopping container zookeeper
04:57:20 Pod test-cluster-2 3724:	Successfully assigned acto-namespace/test-cluster-2 to acto-cluster-8-worker2
04:57:20 Pod test-cluster-2 3725:	Container image "pravega/zookeeper:0.2.14" already present on machine
04:57:20 Pod test-cluster-2 3725:	Created container zookeeper
04:57:21 Pod test-cluster-2 3725:	Started container zookeeper
05:00:12 Pod test-cluster-2 3725:	Stopping container zookeeper
05:00:22 Pod test-cluster-2 3725:	Exec lifecycle hook ([zookeeperTeardown.sh]) for Container "zookeeper" in Pod "test-cluster-2_acto-namespace(302840af-ad30-4072-89e1-9265d7d2d028)" failed - error: command 'zookeeperTeardown.sh' exited with 137: + source /conf/env.sh
++ DOMAIN=test-cluster-headless.acto-namespace.svc.cluster.local
++ QUORUM_PORT=2888
++ LEADER_PORT=3888
++ CLIENT_HOST=test-cluster-client
++ CLIENT_PORT=2181
++ ADMIN_SERVER_HOST=test-cluster-admin-server
++ ADMIN_SERVER_PORT=8080
++ CLUSTER_NAME=test-cluster
++ CLUSTER_SIZE=3
+ source /usr/local/bin/zookeeperFunctions.sh
++ set -ex
+ DATA_DIR=/data
+ MYID_FILE=/data/myid
+ LOG4J_CONF=/conf/log4j-quiet.properties
++ echo cons
++ nc localhost 2181
++ grep -v '^$'
++ grep -v /127.0.0.1:
++ wc -l
+ CONN_COUNT=2
+ (( i = 0 ))
+ (( i < 6 ))
+ [[ 2 -gt 0 ]]
+ echo '2 non-local connections still connected.'
+ sleep 5
+ (( i++  ))
+ (( i < 6 ))
+ [[ 2 -gt 0 ]]
+ echo '2 non-local connections still connected.'
+ sleep 5
, message: "2 non-local connections still connected.\n2 non-local connections still connected.\n+ source /conf/env.sh\n++ DOMAIN=test-cluster-headless.acto-namespace.svc.cluster.local\n++ QUORUM_PORT=2888\n++ LEADER_PORT=3888\n++ CLIENT_HOST=test-cluster-client\n++ CLIENT_PORT=2181\n++ ADMIN_SERVER_HOST=test-cluster-admin-server\n++ ADMIN_SERVER_PORT=8080\n++ CLUSTER_NAME=test-cluster\n++ CLUSTER_SIZE=3\n+ source /usr/local/bin/zookeeperFunctions.sh\n++ set -ex\n+ DATA_DIR=/data\n+ MYID_FILE=/data/myid\n+ LOG4J_CONF=/conf/log4j-quiet.properties\n++ echo cons\n++ nc localhost 2181\n++ grep -v '^$'\n++ grep -v /127.0.0.1:\n++ wc -l\n+ CONN_COUNT=2\n+ (( i = 0 ))\n+ (( i < 6 ))\n+ [[ 2 -gt 0 ]]\n+ echo '2 non-local connections still connected.'\n+ sleep 5\n+ (( i++  ))\n+ (( i < 6 ))\n+ [[ 2 -gt 0 ]]\n+ echo '2 non-local connections still connected.'\n+ sleep 5\n"
05:00:22 Pod test-cluster-2 4154:	Successfully assigned acto-namespace/test-cluster-2 to acto-cluster-8-worker2
05:00:23 Pod test-cluster-2 4155:	Container image "pravega/zookeeper:0.2.14" already present on machine
05:00:23 Pod test-cluster-2 4155:	Created container zookeeper
05:00:23 Pod test-cluster-2 4155:	Started container zookeeper
04:36:14 StatefulSet test-cluster 807:	create Claim data-test-cluster-0 Pod test-cluster-0 in StatefulSet test-cluster success
04:36:14 StatefulSet test-cluster 807:	create Pod test-cluster-0 in StatefulSet test-cluster successful
04:36:39 StatefulSet test-cluster 843:	create Claim data-test-cluster-1 Pod test-cluster-1 in StatefulSet test-cluster success
04:36:39 StatefulSet test-cluster 843:	create Pod test-cluster-1 in StatefulSet test-cluster successful
04:37:07 StatefulSet test-cluster 951:	create Claim data-test-cluster-2 Pod test-cluster-2 in StatefulSet test-cluster success
04:37:07 StatefulSet test-cluster 951:	create Pod test-cluster-2 in StatefulSet test-cluster successful
04:40:21 StatefulSet test-cluster 1361:	delete Pod test-cluster-2 in StatefulSet test-cluster successful
04:41:14 StatefulSet test-cluster 1443:	delete Pod test-cluster-1 in StatefulSet test-cluster successful
04:42:07 StatefulSet test-cluster 1574:	delete Pod test-cluster-0 in StatefulSet test-cluster successful
04:36:11 Pod zkapp 787:	Successfully assigned acto-namespace/zkapp to acto-cluster-8-worker2
04:36:11 Pod zkapp 788:	Container image "tylergu1998/zkapp:v1" already present on machine
04:36:11 Pod zkapp 788:	Created container zkapp
04:36:12 Pod zkapp 788:	Started container zkapp
